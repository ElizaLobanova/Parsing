{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5a892e",
   "metadata": {},
   "source": [
    "Шаблон для запуска парсинга: \\\n",
    "python parsing.py 2 True housedorf urls_housedorf.txt example.xlsx result_housedorf.xlsx \n",
    "python parsing.py 2 False falmec urls_falmec.txt result_housedorf.xlsx result.xlsx\n",
    "\n",
    "python generate_syn_report.py housedorf syn_report_housedorf.xlsx\n",
    "python generate_syn_report.py falmec syn_report_falmec.xlsx\n",
    "\n",
    "python synonyms_dict_update.py syn_report_housedorf.xlsx\n",
    "python synonyms_dict_update.py syn_report_falmec.xlsx\n",
    "\n",
    "python parsing.py 2 True housedorf urls_housedorf.txt example.xlsx result_housedorf.xlsx \n",
    "\n",
    "python parsing.py 2 False falmec urls_falmec.txt result_housedorf.xlsx result.xlsx\n",
    "\n",
    "python compare.py falmec housedorf\n",
    "\n",
    "python parsing.py 24 False smeg urls_smeg.txt result.xlsx result.xlsx \n",
    "python compare.py smeg housedorf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56fe22",
   "metadata": {},
   "source": [
    "python parsing.py 2 True housedorf urls_housedorf.txt articles.xlsx result_housedorf.xlsx \n",
    "python parsing.py 2 False franke_dealer urls_franke-dealer.txt result_gerdamix.xlsx result_franke.xlsx\n",
    "python compare.py blanco housedorf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf61f80",
   "metadata": {},
   "source": [
    "# Программа parsing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f4adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import PatternFill, Alignment\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import os\n",
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "synonyms_path='synonyms.txt'\n",
    "all_characteristics_path='all_characteristics.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50bc4f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'; new_str'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_str = ''\n",
    "full_str = '; '.join([full_str, 'new_str'])\n",
    "full_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17c34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------Спарсить данные------------------------------------------------------------------------\n",
    "\n",
    "# Функции для парсинга\n",
    "def parse_korting_page(html_code):\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    tabs_lists = soup.find_all('ul', class_='tabs-settings__list')\n",
    "    data = {}\n",
    "\n",
    "    for ul in tabs_lists:\n",
    "        for li in ul.find_all('li'):\n",
    "            text = li.get_text(strip=True, separator=\"; \")\n",
    "            split_text = text.split(\":;\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                key, value = split_text\n",
    "                data[key.strip()] = value.strip()\n",
    "            else:\n",
    "                data[split_text[0].strip()] = \"\"\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_dedietrich_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='characteristics__row'):\n",
    "        name_span = row.find('span', class_='characteristics__name')\n",
    "        value_span = row.find('span', class_='characteristics__property')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_granfest_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='item__desc-char'):\n",
    "        name_span = row.find('div', class_='item__desc-char-name')\n",
    "        value_span = row.find('div', class_='item__desc-char-value')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_topzero_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    row = soup.find('div', class_='item-desc-size')\n",
    "    if row:\n",
    "        key, value = ''.join(row.find_all(text=True, recursive=True)).split(':')\n",
    "        if key and value:\n",
    "            key = key.strip()\n",
    "            value = value.strip().replace('\\n', '; ')\n",
    "            data[key] = value\n",
    "    items = soup.find('div', class_='item-desc-other')\n",
    "\n",
    "    for row in items.find_all('div', recursive=False):\n",
    "        full_text = ''.join(row.find_all(text=True, recursive=True))\n",
    "        if ':' in full_text:\n",
    "            key, value = full_text.split(':')\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip().replace('\\n', '; ')\n",
    "                data[key] = value\n",
    "        else:\n",
    "            if not 'Особенности' in data.keys():\n",
    "                data['Особенности'] = full_text.replace('Особенности', '').strip().replace('\\n', '; ')\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_mypremial_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    info_div = soup.find('div', class_='the-item__info')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in info_div.find_all('p'):\n",
    "        name_span = row.find('span', class_='info__title')\n",
    "        value_span = row.find('span', class_='info__value')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip().split(':')[0]\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_gerdamix_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    info_div = soup.find('div', class_='col-lg-5 col-md-5 col-sm-5 col-xs-12 all_opisanie plusi')\n",
    "\n",
    "    full_str = ''\n",
    "    for row in info_div.find_all('li', recursive=True):\n",
    "        new_str = row.find(text=True, recursive=False).strip()\n",
    "        if full_str != '':\n",
    "            full_str = '; '.join([full_str, new_str])\n",
    "        else:\n",
    "            full_str = new_str\n",
    "    \n",
    "    data['Особенности'] = full_str\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_ukinox_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    info_div = soup.find('div', class_='single-har')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in info_div.find_all('li'):\n",
    "        name_span, value_span = row.find_all('p')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip().split(':')[0]\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_rivelato_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    info_div = soup.find('ul', class_='params-list')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in info_div.find_all('li'):\n",
    "        name_span, value_span = row.find_all('span')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=True)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_aquaphor_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('li', class_='equipment-row'):\n",
    "        name_span = row.find('span', class_='equipment-row__name')\n",
    "        value_span = row.find('span', class_='equipment-row__value')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = ''.join(name_span.find_all(text=True, recursive=True)).strip()\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_makmart_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='prop'):\n",
    "        name_span = row.find('div', class_='name')\n",
    "        value_span = row.find('div', class_='value')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.get_text(strip=True)\n",
    "            value = value_span.get_text(strip=True)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_longran_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    row = soup.find('div', class_='catalog-single-har-right')\n",
    "\n",
    "    if row:\n",
    "        for li in row.find_all('li'):\n",
    "            name_span, value_span = li.find_all('p')\n",
    "            \n",
    "            if name_span and value_span:\n",
    "                key = name_span.find(text=True, recursive=False).split(\":\")[0]\n",
    "                value = value_span.find(text=True, recursive=False)\n",
    "                if key and value:\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    data[key] = value\n",
    "    \n",
    "    for row in soup.find_all('div', class_='new-product-specifications__item'):\n",
    "        name_span, value_span = row.find_all('div')\n",
    "        if name_span and value_span:\n",
    "                key = name_span.find(text=True, recursive=False).split(\":\")[0]\n",
    "                value = value_span.find(text=True, recursive=False)\n",
    "                if key and value:\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_fashun_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    rough = soup.find('div', class_='js-store-prod-all-text')\n",
    "\n",
    "    # Разделяем содержимое по тегам <br>\n",
    "    if rough:\n",
    "        parts = []\n",
    "        current = []\n",
    "        for elem in rough.children:\n",
    "            if getattr(elem, 'name', None) == 'br':\n",
    "                text = ''.join(current).strip()\n",
    "                if text:\n",
    "                    parts.append(text)\n",
    "                current = []\n",
    "            else:\n",
    "                current.append(str(elem))\n",
    "        # Добавляем последний кусок, если он есть\n",
    "        if current:\n",
    "            text = ''.join(current).strip()\n",
    "            if text:\n",
    "                parts.append(text)\n",
    "\n",
    "        str_parts = [BeautifulSoup(t, 'html.parser').get_text(strip=True) for t in parts]\n",
    "        for part in str_parts:\n",
    "            if ':' in part:\n",
    "                key, value = part.split(':', 1)\n",
    "                data[key.strip()] = value.strip()\n",
    "            if '=' in part:\n",
    "                key, value = part.split('=', 1)\n",
    "                data[key.strip()] = value.strip()\n",
    "\n",
    "    charcs = soup.find('div', class_='js-store-prod-all-charcs')\n",
    "    if charcs:\n",
    "        for row in charcs.find_all('p'):\n",
    "            key, value = row.find(text=True, recursive=False).split(':', 1)\n",
    "            data[key.strip()] = value.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_blanco_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    rough = soup.find('table', class_='product-attributes')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in rough.find_all('tr'):\n",
    "        name_th = row.find('th')\n",
    "        value_th = row.find('td')\n",
    "        \n",
    "        if name_th and value_th:\n",
    "            key = name_th.find(text=True, recursive=False)\n",
    "            value = value_th.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_geizer_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    rough = soup.find('table', class_='attribute')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for tbody in rough.find_all('tbody'):\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            tds = tr.find_all('td')\n",
    "            if len(tds) == 2:\n",
    "                name_td, value_td = tds\n",
    "                key = name_td.get_text(strip=True)\n",
    "                value = value_td.get_text(strip=True)\n",
    "                if key and value:\n",
    "                    data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_vzug_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    names = soup.find_all('td', class_='cell_name')\n",
    "    values = soup.find_all('td', class_='cell_value')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for name, value in zip(names, values):\n",
    "        name_span = name.find('span')\n",
    "        value_span = value.find('span')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_asco_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='accordeon__item'):\n",
    "        name_span = row.find('span', class_='accordeon__item-title')\n",
    "        value_span = row.find('p', class_='accordeon__item-text')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_kuppersbush_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='wdu_propsorter'):\n",
    "        for item in row.find_all('tr'):\n",
    "            tds = item.find_all('td')\n",
    "\n",
    "            if tds:\n",
    "                name_td, value_td = tds\n",
    "                key = name_td.get_text(strip=True)\n",
    "                value = value_td.get_text(strip=True)\n",
    "                if key and value:\n",
    "                    data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_shaublorenz_shop_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('tr', class_='tablerow'):\n",
    "        text = row.find_all(text=True, recursive=True)\n",
    "        \n",
    "        if text:\n",
    "            key = text[0]\n",
    "            value = text[2]\n",
    "            data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_shaublorenz_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='attr-group-box'):\n",
    "        name_div = row.find('div', class_='attr-name')\n",
    "        value_div = row.find('div', class_='attr-value')\n",
    "        \n",
    "        if name_div and value_div:\n",
    "            key = name_div.find(text=True, recursive=False)\n",
    "            value = value_div.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_evelux_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='product__content-specs-line'):\n",
    "        name_span = row.find('div', class_='product__content-specs-title')\n",
    "        value_span = row.find('div', class_='product__content-specs-subtitle')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False).split(':')[0]\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_graude_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for name_td, value_td in zip(soup.find_all('td', class_='cell_name'), soup.find_all('td', class_='cell_value')):\n",
    "        key = name_td.find(text=True, recursive=True)\n",
    "        value = value_td.find(text=True, recursive=True)\n",
    "        if key and value:\n",
    "            key = key.strip().split(':')[0]\n",
    "            value = value.strip()\n",
    "            data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_franke_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='cmp-product-information-table__section-list__item'):\n",
    "        name_span = row.find('span', class_='cmp-product-information-table__section-list__item__name')\n",
    "        value_span = row.find('span', class_='cmp-product-information-table__section-list__item__value')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_smeg_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('tr', class_='s-feature-column'):\n",
    "        name_td = row.find('td', class_='name')\n",
    "        value_td = row.find('td', class_='value')\n",
    "        \n",
    "        if name_td and value_td:\n",
    "            key = name_td.find(text=True, recursive=True)\n",
    "            value = value_td.find(text=True, recursive=True)\n",
    "            if key and value and \"фид\" not in key:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_history_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('table', class_='alignleft'):\n",
    "        for tr in row.find_all('tr'):\n",
    "            tds = tr.find_all('td')\n",
    "            \n",
    "            if len(tds) == 2:\n",
    "                name_td, value_td = tds\n",
    "                key = name_td.find(text=True, recursive=False)\n",
    "                value = value_td.find(text=True, recursive=False)\n",
    "                if key and value:\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_elica_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='ty-product-feature'):\n",
    "        name_span = row.find('span', class_='ty-product-feature__label')\n",
    "        value_div = row.find('div', class_='ty-product-feature__value')\n",
    "        \n",
    "        if name_span and value_div:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_div.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip().rstrip('.,;!?—:').split('\\n')[0]\n",
    "                value = value.strip().rstrip('.,;!?—:').split('\\n')[0]\n",
    "                data[key] = value\n",
    "            elif key:\n",
    "                checkbox = value_div.find('span', class_='ty-compare-checkbox')\n",
    "                title = checkbox.get('title')\n",
    "                if title == 'Y':\n",
    "                    value = \"Да\"\n",
    "                    data[key.rstrip('.,;!?—:').split('\\n')[0]] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_franke_dealer_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('tr'):\n",
    "        name_td = row.find('td', class_='name')\n",
    "        value_td = row.find('td', class_='value')\n",
    "        \n",
    "        if name_td and value_td:\n",
    "            key = name_td.find(text=True, recursive=False)\n",
    "            value = value_td.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_konigin_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # 1. Извлекаем div.column и все p внутри\n",
    "    column_div = soup.find('div', class_='column')\n",
    "    if column_div:\n",
    "        p_tags = column_div.find_all('p')\n",
    "        # Игнорируем первые 2 и последние 3\n",
    "        middle_p_tags = p_tags[2:-3]\n",
    "\n",
    "        # Соединяем текст через \"; \"\n",
    "        additional_info = \"; \".join(p.get_text(strip=True).rstrip('.,;!?—:') for p in middle_p_tags)\n",
    "        data['ДОПОЛНИТЕЛЬНАЯ ИНФОРМАЦИЯ'] = additional_info\n",
    "\n",
    "        # Обрабатываем последние два тега <p>\n",
    "        for p in p_tags[-2:]:\n",
    "            text = p.get_text(strip=True)\n",
    "            # Делим по точкам\n",
    "            for part in text.split('.'):\n",
    "                if ':' in part:\n",
    "                    key, value = part.split(':', 1)\n",
    "                    data[key.strip()] = value.strip()\n",
    "\n",
    "    # 2. Обрабатываем блоки div.card-spec__item\n",
    "    spec_items = soup.find_all('div', class_='card-spec__item')\n",
    "    for item in spec_items:\n",
    "        title_div = item.find('div', class_='card-spec__item-title')\n",
    "        info_div = item.find('div', class_='card-spec__item-info')\n",
    "        if title_div and info_div:\n",
    "            key = title_div.get_text(strip=True)\n",
    "            value = info_div.get_text(strip=True).rstrip('.,;!?—:')\n",
    "            data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_falmec_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='characteristics__row'):\n",
    "        name_span = row.find('span', class_='characteristics__name')\n",
    "        value_span = row.find('span', class_='characteristics__property')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "\n",
    "            # Если нет простого текста, ищем <ul> и собираем <li>\n",
    "            if (value == '' or not value.strip()) and value_span.find('ul'):\n",
    "                li_items = value_span.find_all('li')\n",
    "                value = '; '.join(li.get_text(strip=True) for li in li_items)\n",
    "\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_first_visible_text(tag):\n",
    "    for desc in tag.descendants:\n",
    "        if isinstance(desc, str):  # Это NavigableString\n",
    "            text = desc.strip()\n",
    "            if text:\n",
    "                return text\n",
    "    return None\n",
    "\n",
    "def clean_value_div(value_div):\n",
    "    # 1. Удалить все <span>\n",
    "    for span in value_div.find_all(\"span\"):\n",
    "        span.decompose()\n",
    "\n",
    "    # 2. Разделить по <br> — создаём список на основе HTML с разделителем\n",
    "    parts = str(value_div).split('<br')\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for part_html in parts:\n",
    "        # Восстанавливаем HTML-тег <br>, если он был отрезан\n",
    "        if not part_html.startswith('>'):\n",
    "            part_html = '<br' + part_html\n",
    "\n",
    "        part_soup = BeautifulSoup(part_html, 'html.parser')\n",
    "\n",
    "        # 3. Найти первый видимый текст\n",
    "        for desc in part_soup.descendants:\n",
    "            if isinstance(desc, NavigableString):\n",
    "                text = desc.strip()\n",
    "                if text:\n",
    "                    values.append(text)\n",
    "                    break  # только первое вхождение\n",
    "\n",
    "    # 4. Склеить с разделителем \"; \"\n",
    "    return \"; \".join(values)\n",
    "\n",
    "def parse_hausedorf_page(html_code):\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    fields = soup.find_all('div', class_='detail-properties__field')\n",
    "    data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        name_div = field.find('div', class_='detail-properties__name')\n",
    "        value_div = field.find('div', class_='detail-properties__value')\n",
    "\n",
    "        if name_div and value_div:\n",
    "            key = extract_first_visible_text(name_div)\n",
    "            value = clean_value_div(value_div)\n",
    "\n",
    "            if key and value:\n",
    "                data[re.sub(r'\\s+', ' ', key).strip()] = value.replace(\">\\n\", \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_src(file_path, parser_func):\n",
    "    \"\"\"\n",
    "    Универсальный загрузчик таблицы характеристик с разных сайтов.\n",
    "\n",
    "    :param file_path: путь к файлу с URL (один URL на строку)\n",
    "    :param parser_func: функция, которая получает HTML-код и возвращает словарь {ключ: значение}\n",
    "    :return: DataFrame с объединёнными результатами\n",
    "    \"\"\"\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        urls = [line.strip() for line in f]\n",
    "\n",
    "    n_rows = 0\n",
    "    for url in tqdm(urls):\n",
    "        if url:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.encoding = 'utf-8'\n",
    "                # response.encoding = response.apparent_encoding\n",
    "                html_code = response.text\n",
    "                data = parser_func(html_code) \n",
    "\n",
    "                if not isinstance(data, dict):\n",
    "                    raise ValueError(\"parser_func должна возвращать словарь!\")\n",
    "\n",
    "                row_df = pd.DataFrame([data])\n",
    "                df_all = pd.concat([df_all, row_df], ignore_index=True)\n",
    "\n",
    "                if len(df_all) == 1:\n",
    "                    empty_rows = pd.DataFrame(np.nan, index=range(n_rows), columns=df_all.columns)\n",
    "                    df_all = pd.concat([empty_rows, df_all], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке {url}: {e}\")\n",
    "        else:\n",
    "            if len(df_all.columns) > 0:                \n",
    "                df_all.loc[len(df_all)] = None\n",
    "            else:\n",
    "                n_rows += 1\n",
    "\n",
    "    return df_all.where(pd.notnull(df_all), None)\n",
    "\n",
    "# ---------------------------------------------Записать и вернуть дополненный названиями номенклатуры DataFrame---------------------------------------------\n",
    "\n",
    "def write_dest(ref_file_path, result_file_path, df_src, start_row_index):\n",
    "    # Путь к файлу Excel\n",
    "    wb = load_workbook(ref_file_path)\n",
    "    ws = wb.active  # или wb['SheetName']\n",
    "\n",
    "    # Поля файла-приёмника\n",
    "    row_header = [cell.value for cell in ws[1]]\n",
    "\n",
    "    # Сопоставление колонок\n",
    "    src_cols_lower = {col.lower(): col for col in df_src.columns}\n",
    "    ws_cols_lower = {i: str(header).strip().lower() if header else \"\" for i, header in enumerate(row_header)}\n",
    "    matched_columns = []\n",
    "    common_cols = set()\n",
    "    nomenclature_col_idx = None\n",
    "\n",
    "    for col_idx, header_lower in ws_cols_lower.items():\n",
    "        if header_lower == \"номенклатура\":\n",
    "            nomenclature_col_idx = col_idx\n",
    "        if header_lower in src_cols_lower:\n",
    "            matched_columns.append((col_idx, src_cols_lower[header_lower]))\n",
    "            common_cols.add(src_cols_lower[header_lower])\n",
    "\n",
    "    if nomenclature_col_idx is None:\n",
    "        raise ValueError(\"Колонка 'Номенклатура' не найдена в файле-приёмнике\")\n",
    "\n",
    "    # Считываем значения \"Номенклатура\"\n",
    "    nomenclature_values = []\n",
    "    for i in range(len(df_src)):\n",
    "        cell_value = ws.cell(row=start_row_index + i, column=nomenclature_col_idx + 1).value\n",
    "        nomenclature_values.append(cell_value)\n",
    "\n",
    "    # Запись данных\n",
    "    for i, (_, row_src) in enumerate(df_src.iterrows()):\n",
    "        for col_idx, src_col in matched_columns:\n",
    "            cell = ws.cell(row=start_row_index + i, column=col_idx + 1)\n",
    "            if cell.value in [None, \"\"]:\n",
    "                cell.value = row_src[src_col]\n",
    "\n",
    "    # Сохранение\n",
    "    wb.save(result_file_path)\n",
    "\n",
    "    # Подготовка выходного DataFrame\n",
    "    result_df = df_src[list(common_cols)].copy()\n",
    "    result_df.insert(0, \"Номенклатура\", pd.Series(nomenclature_values))\n",
    "\n",
    "    return result_df, common_cols\n",
    "\n",
    "# -------------------------------------------Сохранить незаписанные данные в дополнительные колонки или отдельный файл-------------------------------------------\n",
    "\n",
    "def append_dataframe_to_excel(df: pd.DataFrame, file_path: str, result_path: str, start_row: int):\n",
    "    # Проверка, существует ли файл\n",
    "    if os.path.exists(file_path):\n",
    "        wb = load_workbook(file_path)\n",
    "    else:\n",
    "        # Если файла нет, создаем новый\n",
    "        wb = Workbook()\n",
    "    \n",
    "    ws = wb.active\n",
    "\n",
    "    # Найдём первую пустую ячейку в первой строке\n",
    "    col_index = 1\n",
    "    while ws.cell(row=1, column=col_index).value is not None:\n",
    "        col_index += 1\n",
    "\n",
    "    # Записываем названия колонок DataFrame в первую строку, начиная с найденной колонки\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        cell = ws.cell(row=1, column=col_index + i, value=col_name)\n",
    "        if len(col_name.split('_')) > 1:\n",
    "            cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')\n",
    "            \n",
    "\n",
    "    # Автонастройка ширины столбцов по первой строке\n",
    "    for col_idx, cell in enumerate(ws[1], start=col_index):\n",
    "        max_length = len(str(cell.value)) if cell.value else 0\n",
    "        col_letter = cell.column_letter\n",
    "        ws.column_dimensions[col_letter].width = max_length + 2  # +2 для отступа\n",
    "\n",
    "    # Применение стилей и переносов\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, max_col=ws.max_column):\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical='center')  # включаем перенос текста\n",
    "\n",
    "    # Записываем данные DataFrame начиная со start_row\n",
    "    for row_offset, row in enumerate(dataframe_to_rows(df, index=False, header=False)):\n",
    "        for i, value in enumerate(row):\n",
    "            ws.cell(row=start_row + row_offset, column=col_index + i, value=value)\n",
    "\n",
    "    # Сохраняем файл\n",
    "    wb.save(result_path)\n",
    "\n",
    "def save_missing(df1, filepath):\n",
    "\n",
    "    # Создаём ExcelWriter\n",
    "    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "        row = 0\n",
    "\n",
    "        # Запись df1\n",
    "        df1.to_excel(writer, index=False, startrow=row)\n",
    "        row += len(df1) + 2  # +1 за заголовок, +1 за пустую строку\n",
    "\n",
    "        # Автоматическая установка ширины колонок\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        for column_cells in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column = column_cells[0].column\n",
    "            for cell in column_cells:\n",
    "                if cell.value:\n",
    "                    max_length = max(max_length, len(str(cell.value)))\n",
    "            adjusted_width = max_length + 2\n",
    "            worksheet.column_dimensions[get_column_letter(column)].width = adjusted_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb24f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Особенности': 'латунь; выход для питьевой воды; высокий поворотный излив; однорычажный; керамический картридж 35 мм; гибкая подводка 400 мм; гарантия 3 года'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Протестировать функцию парсинга\n",
    "def test_parser(url, parser_func):\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'\n",
    "    # response.encoding = response.apparent_encoding\n",
    "    html_code = response.text\n",
    "    data = parser_func(html_code)\n",
    "    return data\n",
    "\n",
    "data = test_parser('https://gerdamix.ru/dorado/', parse_gerdamix_page)\n",
    "print(len(data))  # Должно вернуть количество характеристик, например 10\n",
    "data # Выводим результат парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64a089da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def list_synonyms_comparison(list1, list2):\n",
    "    return [are_synonyms(word1, word2) for word1, word2 in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "764e71c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, False]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Протестировать подбор синонимов\n",
    "list1 = ['Машина', 'Счастливый', 'Работать']\n",
    "list2 = ['автомобиля', 'счастливый', 'работник']\n",
    "list_synonyms_comparison(list1, list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571ce816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------Добавить функцию для обеспечения правильного дописывания данных основываясь на утверждённых синонимах-----------------------------\n",
    "\n",
    "def parse_custom_dict_line(line):\n",
    "    \"\"\"\n",
    "    Разбирает строку из словаря: <характеристика>: <синоним1>; <синоним2>; ...| <антисиноним1>, <антисиноним2>, ...\n",
    "    \"\"\"\n",
    "    base, *rest = line.strip().split(':')\n",
    "    if not rest:\n",
    "        return base.strip(), set(), set()\n",
    "    syn_ant = rest[0].split('|')\n",
    "    synonyms = set(map(str.strip, syn_ant[0].split(';'))) if syn_ant[0] else set()\n",
    "    antisynonyms = set(map(str.strip, syn_ant[1].split(','))) if len(syn_ant) > 1 else set()\n",
    "    return base.strip(), synonyms, antisynonyms\n",
    "\n",
    "def load_existing_synonyms(file_path):\n",
    "    syn_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            base, synonyms, antisynonyms = parse_custom_dict_line(line)\n",
    "            syn_dict[base] = {'synonyms': synonyms, 'antisynonyms': antisynonyms}\n",
    "    return syn_dict\n",
    "\n",
    "def rename_columns_with_syn_dict(df, syn_dict_path, all_1c_chars_path):\n",
    "    # Загрузка словаря\n",
    "    synonyms_dict = load_existing_synonyms(syn_dict_path)\n",
    "    all_chars = pd.read_excel(all_1c_chars_path, header=None).iloc[0].astype(str).tolist()\n",
    "    all_chars_lower = [char.lower() for char in all_chars]  # Приводим к нижнему регистру для сравнения\n",
    "\n",
    "    # Удаление дублирующихся колонок по имени\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # Поиск синонимичных имен колонок через словарь\n",
    "    df_expanded = df.copy()\n",
    "    unsyn_set = set()\n",
    "    for col in df.columns:\n",
    "        col_splitted = col.split(':')[0]\n",
    "        # Если колонка уже есть в 1С - пропускаем её\n",
    "        if col_splitted.lower() in all_chars_lower:\n",
    "            df_expanded[col_splitted.upper()] = df[col]\n",
    "            df_expanded.drop(columns=[col], inplace=True)\n",
    "            continue\n",
    "\n",
    "        is_syn = False\n",
    "        for synonym_key, syn_data in synonyms_dict.items():\n",
    "            syn_set = syn_data.get(\"synonyms\", set())\n",
    "            if col_splitted in syn_set:\n",
    "                # Добавляем колонку с именем synonym_key, если она ещё не существует\n",
    "                if synonym_key not in df_expanded.columns:\n",
    "                    df_expanded[synonym_key] = df[col]\n",
    "                    is_syn = True\n",
    "        if is_syn:\n",
    "            # Если колонка была переименована, удаляем оригинальную и не ищем полусинонимы\n",
    "            df_expanded.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            is_half_syn = False\n",
    "            for synonym_key, syn_data in synonyms_dict.items():\n",
    "                syn_set = syn_data.get(\"synonyms\", set())\n",
    "                if col_splitted in set(map(lambda x: x.split(\"*\")[1] if len(x.split(\"*\")) > 1 else x, syn_set)):\n",
    "                    # Добавляем колонку с именем synonym_key, если она ещё не существует\n",
    "                    if synonym_key not in df_expanded.columns:\n",
    "                        df_expanded[f\"{synonym_key}_{col_splitted}\"] = df[col]\n",
    "                        is_half_syn = True\n",
    "\n",
    "            if is_half_syn:\n",
    "                df_expanded.drop(columns=[col], inplace=True)\n",
    "            else:\n",
    "                is_antisyn = False\n",
    "                for synonym_key, syn_data in synonyms_dict.items():\n",
    "                    antisyn_set = syn_data.get(\"antisynonyms\", set())\n",
    "                    if col_splitted in antisyn_set:\n",
    "                        is_antisyn = True\n",
    "                        break\n",
    "\n",
    "                if not is_antisyn:\n",
    "                    unsyn_set.add(col_splitted)\n",
    "\n",
    "    return df_expanded, unsyn_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6bcdd",
   "metadata": {},
   "source": [
    "# Варианты запуска парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194aa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------Запуск парсинга и сохранение результатов-----------------------------------------------------------\n",
    "\n",
    "# Протестировать запись одной строки с двух сайтов\n",
    "from argparse import Namespace\n",
    "\n",
    "# housedorf\n",
    "args = Namespace(start_row=33, append=True, site='housedorf', urls_source='input_example/url_housedorf.txt', input_path='input_example/example.xlsx', output_path='result_housedorf.xlsx')\n",
    "if not args.output_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "# Проверка, не повреждены ли входные файлы\n",
    "wb = load_workbook(args.input_path)\n",
    "\n",
    "if args.site == 'korting':\n",
    "    df_src = create_src(args.urls_source, parse_korting_page)\n",
    "elif args.site == 'housedorf':\n",
    "    df_src = create_src(args.urls_source, parse_hausedorf_page)\n",
    "elif args.site == 'dedietrich':\n",
    "    df_src = create_src(args.urls_source, parse_dedietrich_page)\n",
    "elif args.site == 'falmec':\n",
    "    df_src = create_src(args.urls_source, parse_falmec_page)\n",
    "elif args.site == 'vzug':\n",
    "    df_src = create_src(args.urls_source, parse_vzug_page)\n",
    "else:\n",
    "    raise ValueError(\"There're no parse function for this site\")\n",
    "\n",
    "df_src, unsyn_set = rename_columns_with_syn_dict(df_src, synonyms_path, all_characteristics_path) # Для обеспечения правильного дописывания данных и для корректного входа к функции, \n",
    "                                                                        # генерирующей отчёт\n",
    "resultdf, com_cols = write_dest(args.input_path, args.output_path, df_src, args.start_row)\n",
    "resultdf.to_parquet(f\"{args.site}_auxiliary.parquet\")\n",
    "\n",
    "com_cols = resultdf.columns.intersection(df_src.columns)\n",
    "missingdf = df_src.drop(columns=com_cols).copy()\n",
    "print(unsyn_set)\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'w') as f:\n",
    "    f.write('; '.join(map(str, unsyn_set)))\n",
    "\n",
    "if args.append:\n",
    "    append_dataframe_to_excel(missingdf, args.output_path, args.output_path, args.start_row)\n",
    "else:\n",
    "    missingdf.insert(0, 'Номенклатура', resultdf['Номенклатура'].copy())\n",
    "    save_missing(missingdf, f'missing_{args.site}.xlsx')\n",
    "\n",
    "# korting\n",
    "args = Namespace(start_row=34, append=False, site='korting', urls_source='input_example/url_korting.txt', input_path='result_housedorf.xlsx', output_path='result.xlsx')\n",
    "if not args.output_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "# Проверка, не повреждены ли входные файлы\n",
    "wb = load_workbook(args.input_path)\n",
    "\n",
    "if args.site == 'korting':\n",
    "    df_src = create_src(args.urls_source, parse_korting_page)\n",
    "elif args.site == 'housedorf':\n",
    "    df_src = create_src(args.urls_source, parse_hausedorf_page)\n",
    "elif args.site == 'dedietrich':\n",
    "    df_src = create_src(args.urls_source, parse_dedietrich_page)\n",
    "elif args.site == 'falmec':\n",
    "    df_src = create_src(args.urls_source, parse_falmec_page)\n",
    "elif args.site == 'vzug':\n",
    "    df_src = create_src(args.urls_source, parse_vzug_page)\n",
    "else:\n",
    "    raise ValueError(\"There're no parse function for this site\")\n",
    "\n",
    "df_src, unsyn_set = rename_columns_with_syn_dict(df_src, synonyms_path, all_characteristics_path) # Для обеспечения правильного дописывания данных и для корректного входа к функции, \n",
    "                                                                        # генерирующей отчёт\n",
    "resultdf, com_cols = write_dest(args.input_path, args.output_path, df_src, args.start_row)\n",
    "resultdf.to_parquet(f\"{args.site}_auxiliary.parquet\")\n",
    "\n",
    "com_cols = resultdf.columns.intersection(df_src.columns)\n",
    "missingdf = df_src.drop(columns=com_cols).copy()\n",
    "print(unsyn_set)\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'w') as f:\n",
    "    f.write('; '.join(map(str, unsyn_set)))\n",
    "\n",
    "if args.append:\n",
    "    append_dataframe_to_excel(missingdf, args.output_path, args.output_path, args.start_row)\n",
    "else:\n",
    "    missingdf.insert(0, 'Номенклатура', resultdf['Номенклатура'].copy())\n",
    "    save_missing(missingdf, f'missing_{args.site}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ef97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Актуальная версия запуска\n",
    "from argparse import Namespace\n",
    "\n",
    "# housedorf\n",
    "args = Namespace(start_row=7, append=False, site='falmec', urls_source='url_falmec.txt', input_path='result_housedorf.xlsx', output_path='result.xlsx')\n",
    "if not args.output_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "# Проверка, не повреждены ли входные файлы\n",
    "wb = load_workbook(args.input_path)\n",
    "\n",
    "if args.site == 'korting':\n",
    "    df_src = create_src(args.urls_source, parse_korting_page)\n",
    "elif args.site == 'housedorf':\n",
    "    df_src = create_src(args.urls_source, parse_hausedorf_page)\n",
    "elif args.site == 'dedietrich':\n",
    "    df_src = create_src(args.urls_source, parse_dedietrich_page)\n",
    "elif args.site == 'falmec':\n",
    "    df_src = create_src(args.urls_source, parse_falmec_page)\n",
    "elif args.site == 'vzug':\n",
    "    df_src = create_src(args.urls_source, parse_vzug_page)\n",
    "elif args.site == 'asco':\n",
    "    df_src = create_src(args.urls_source, parse_asco_page)\n",
    "elif args.site == 'kuppersbush':\n",
    "    df_src = create_src(args.urls_source, parse_kuppersbush_page)\n",
    "elif args.site == 'konigin':\n",
    "    df_src = create_src(args.urls_source, parse_konigin_page)\n",
    "elif args.site == 'evelux':\n",
    "    df_src = create_src(args.urls_source, parse_evelux_page)\n",
    "elif args.site == 'franke':\n",
    "    df_src = create_src(args.urls_source, parse_franke_page)\n",
    "elif args.site == 'franke_dealer':\n",
    "    df_src = create_src(args.urls_source, parse_franke_dealer_page)\n",
    "elif args.site == 'elica':\n",
    "    df_src = create_src(args.urls_source, parse_elica_page)\n",
    "elif args.site == 'smeg':\n",
    "    df_src = create_src(args.urls_source, parse_smeg_page)\n",
    "elif args.site == 'shaublorenz':\n",
    "    df_src = create_src(args.urls_source, parse_shaublorenz_page)\n",
    "elif args.site == 'shaublorenz_shop':\n",
    "    df_src = create_src(args.urls_source, parse_shaublorenz_shop_page)\n",
    "elif args.site == 'graude':\n",
    "    df_src = create_src(args.urls_source, parse_graude_page)\n",
    "elif args.site == 'history':\n",
    "    df_src = create_src(args.urls_source, parse_history_page)\n",
    "elif args.site == 'blanco':\n",
    "    df_src = create_src(args.urls_source, parse_blanco_page)\n",
    "elif args.site == 'fashun':\n",
    "    df_src = create_src(args.urls_source, parse_fashun_page)\n",
    "elif args.site == 'geizer':\n",
    "    df_src = create_src(args.urls_source, parse_geizer_page)\n",
    "elif args.site == 'longran':\n",
    "    df_src = create_src(args.urls_source, parse_longran_page)\n",
    "elif args.site == 'makmart':\n",
    "    df_src = create_src(args.urls_source, parse_makmart_page)\n",
    "elif args.site == 'aquaphor':\n",
    "    df_src = create_src(args.urls_source, parse_aquaphor_page)\n",
    "elif args.site == 'mypremial':\n",
    "    df_src = create_src(args.urls_source, parse_mypremial_page)\n",
    "elif args.site == 'rivelato':\n",
    "    df_src = create_src(args.urls_source, parse_rivelato_page)\n",
    "elif args.site == 'topzero':\n",
    "    df_src = create_src(args.urls_source, parse_topzero_page)\n",
    "elif args.site == 'ukinox':\n",
    "    df_src = create_src(args.urls_source, parse_ukinox_page)\n",
    "elif args.site == 'granfest':\n",
    "    df_src = create_src(args.urls_source, parse_granfest_page)\n",
    "elif args.site == 'gerdamix':\n",
    "    df_src = create_src(args.urls_source, parse_gerdamix_page)\n",
    "else:\n",
    "    raise ValueError(\"There're no parse function for this site\")\n",
    "\n",
    "df_src, unsyn_set = rename_columns_with_syn_dict(df_src, synonyms_path, all_characteristics_path) # Для обеспечения правильного дописывания данных и для корректного входа к функции, \n",
    "                                                                        # генерирующей отчёт\n",
    "resultdf, com_cols = write_dest(args.input_path, args.output_path, df_src, args.start_row)\n",
    "resultdf.to_parquet(f\"{args.site}_auxiliary.parquet\")\n",
    "\n",
    "com_cols = resultdf.columns.intersection(df_src.columns)\n",
    "missingdf = df_src.drop(columns=com_cols).copy()\n",
    "print(unsyn_set)\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('; '.join(map(str, unsyn_set)))\n",
    "\n",
    "if args.append:\n",
    "    append_dataframe_to_excel(missingdf, args.output_path, args.output_path, args.start_row)\n",
    "else:\n",
    "    # Условие: имена столбцов, у которых есть хотя бы один символ \"_\"\n",
    "    columns_with_underscore = [col for col in missingdf.columns if len(col.split(\"_\")) > 1]\n",
    "    columns_without_underscore = [col for col in missingdf.columns if len(col.split(\"_\")) <= 1]\n",
    "\n",
    "    # Делим missingdf на два\n",
    "    df_with_underscore = missingdf[columns_with_underscore]\n",
    "    df_without_underscore = missingdf[columns_without_underscore]\n",
    "    df_without_underscore.insert(0, 'Номенклатура', resultdf['Номенклатура'].copy())\n",
    "    save_missing(df_without_underscore, f'missing_{args.site}.xlsx')\n",
    "    append_dataframe_to_excel(df_with_underscore, args.output_path, args.output_path, args.start_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc06f5",
   "metadata": {},
   "source": [
    "# Программа generate_syn_report.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2cfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "synonyms_path='synonyms.txt'\n",
    "all_characteristics_path='all_characteristics.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a23905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def list_synonyms_comparison(list1, list2):\n",
    "    return [are_synonyms(word1, word2) for word1, word2 in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cdbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Добавить функцию для создания отчёта о сопоставлении колонок тем, что уже существуют в 1С-----------------------------------\n",
    "\n",
    "def parse_custom_dict_line(line):\n",
    "    \"\"\"\n",
    "    Разбирает строку из словаря: <характеристика>: <синоним1>; <синоним2>; ...| <антисиноним1>, <антисиноним2>, ...\n",
    "    \"\"\"\n",
    "    base, *rest = line.strip().split(':')\n",
    "    if not rest:\n",
    "        return base.strip(), set(), set()\n",
    "    syn_ant = rest[0].split('|')\n",
    "    synonyms = set(map(str.strip, syn_ant[0].split(';'))) if syn_ant[0] else set()\n",
    "    antonyms = set(map(str.strip, syn_ant[1].split(','))) if len(syn_ant) > 1 else set()\n",
    "    return base.strip(), synonyms, antonyms\n",
    "\n",
    "def load_existing_synonyms(file_path):\n",
    "    syn_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            base, synonyms, antonyms = parse_custom_dict_line(line)\n",
    "            syn_dict[base] = {'synonyms': synonyms, 'antonyms': antonyms}\n",
    "    return syn_dict\n",
    "\n",
    "def tokenize(text):\n",
    "    return set(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "def are_words_possibly_synonyms(words1, words2, are_synonims_func):\n",
    "    pairs = product(words1, words2)\n",
    "    for w1, w2 in pairs:\n",
    "        if w1 == w2:\n",
    "            return True\n",
    "        if are_synonims_func(w1, w2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_synonym_report(existing_dict_path, all_1c_chars_path, new_chars, are_synonims_func, output_excel_path):\n",
    "    custom_dict = load_existing_synonyms(existing_dict_path)\n",
    "    all_chars = pd.read_excel(all_1c_chars_path, header=None).iloc[0].astype(str).tolist()\n",
    "    result_rows = []\n",
    "\n",
    "    for c1 in tqdm(all_chars):\n",
    "        for c2 in new_chars:\n",
    "            if c1 == c2:\n",
    "                continue\n",
    "            tokens1 = tokenize(c1)\n",
    "            tokens2 = tokenize(c2)\n",
    "            if are_words_possibly_synonyms(tokens1, tokens2, are_synonims_func):\n",
    "                result_rows.append((c1, c2, None))  # None для ручной отметки\n",
    "\n",
    "    df_result = pd.DataFrame(result_rows, columns=[\"base_char\", \"compared_char\", \"label\"])\n",
    "    df_result.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------Запуск генерации-----------------------------------------------------------------------\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(site='housedorf', synonyms_report_path='syn_report_housedorf.xlsx')\n",
    "if not args.synonyms_report_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "unsyn_list = list(map(str, data.strip().split('; ')))\n",
    "\n",
    "generate_synonym_report(\n",
    "    synonyms_path,\n",
    "    all_characteristics_path,\n",
    "    unsyn_list,\n",
    "    are_synonyms,\n",
    "    args.synonyms_report_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c5bd6",
   "metadata": {},
   "source": [
    "# Программа synonyms_dict_update.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6afd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import argparse\n",
    "synonyms_path='synonyms.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd4a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------Обновить словарь синонимов из Excel--------------------------------------------------------------\n",
    "def load_synonym_dict(dict_path):\n",
    "    syn_dict = defaultdict(lambda: {\"synonyms\": set(), \"antisynonyms\": set()})\n",
    "    if not os.path.exists(dict_path):\n",
    "        return syn_dict\n",
    "    \n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            key, rest = line.strip().split(\":\", 1)\n",
    "            syn_part, *anti_part = rest.strip().split(\"|\")\n",
    "            syns = set(map(str.strip, syn_part.strip().split(\";\"))) if syn_part.strip() else set()\n",
    "            antis = set(map(str.strip, anti_part[0].strip().split(\";\"))) if anti_part and anti_part[0].strip() else set()\n",
    "            syn_dict[key.strip()][\"synonyms\"].update(syns)\n",
    "            syn_dict[key.strip()][\"antisynonyms\"].update(antis)\n",
    "    return syn_dict\n",
    "\n",
    "\n",
    "def save_synonym_dict(syn_dict, dict_path):\n",
    "    with open(dict_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for key in sorted(syn_dict.keys()):\n",
    "            syns = \"; \".join(sorted(syn_dict[key][\"synonyms\"]))\n",
    "            antis = \"; \".join(sorted(syn_dict[key][\"antisynonyms\"]))\n",
    "            line = f\"{key}: {syns} | {antis}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "\n",
    "def update_synonym_dict_from_excel(excel_path, dict_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    if not {\"base_char\", \"compared_char\", \"label\"}.issubset(df.columns):\n",
    "        raise ValueError(\"Excel должен содержать столбцы: base_char, compared_char, label\")\n",
    "    \n",
    "    syn_dict = load_synonym_dict(dict_path)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        base = row[\"base_char\"].strip().split(\":\")[0]\n",
    "        comp = row[\"compared_char\"].strip().split(\":\")[0]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        if label == 1:\n",
    "            syn_dict[base][\"synonyms\"].add(comp)\n",
    "        elif label == 0.5:\n",
    "            syn_dict[base][\"synonyms\"].add(\"*\"+comp)\n",
    "        elif label == 0 or pd.isna(label):\n",
    "            syn_dict[base][\"antisynonyms\"].add(comp)\n",
    "        else:\n",
    "            continue  # Пропустить некорректные значения\n",
    "\n",
    "    # Удалить пересекающиеся значения\n",
    "    for base in syn_dict:\n",
    "        overlap1 = syn_dict[base][\"synonyms\"] & syn_dict[base][\"antisynonyms\"]\n",
    "        overlap2 = set(map(lambda x: x.split(\"*\")[1] if len(x.split(\"*\")) > 1 else x, syn_dict[base][\"synonyms\"])) & syn_dict[base][\"antisynonyms\"]\n",
    "        syn_dict[base][\"antisynonyms\"] -= overlap1\n",
    "        syn_dict[base][\"antisynonyms\"] -= overlap2\n",
    "\n",
    "    save_synonym_dict(syn_dict, dict_path)\n",
    "    print(f\"Обновлённый словарь сохранён в {dict_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d8486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обновлённый словарь сохранён в synonyms.txt\n"
     ]
    }
   ],
   "source": [
    "# Запустить обновление файла-словаря\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "# housedorf\n",
    "args = Namespace(report_path='syn_report_housedorf.xlsx')\n",
    "if not args.report_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "update_synonym_dict_from_excel(args.report_path, synonyms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестировать обновление файла-словаря\n",
    "syn_dict = load_synonym_dict('synonyms.txt')\n",
    "for base in syn_dict:\n",
    "    if len(syn_dict[base]['synonyms']) > 0:\n",
    "        print(f\"{base}: {syn_dict[base]['synonyms']}, {syn_dict[base]['antisynonyms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee85889",
   "metadata": {},
   "source": [
    "# Программа compare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import argparse\n",
    "from itertools import product\n",
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f42b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def tokenize(text):\n",
    "    return set(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "def are_words_possibly_synonyms(words1, words2, are_synonims_func):\n",
    "    pairs = product(words1, words2)\n",
    "    for w1, w2 in pairs:\n",
    "        if w1 == w2:\n",
    "            return True\n",
    "        if are_synonims_func(w1, w2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b87f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------Сравнить записанные данные-------------------------------------------------------------------\n",
    "\n",
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, name1: str = 'df1', name2: str = 'df2') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Сравнивает два DataFrame по общим столбцам, включая \"Номенклатура\" как ключ.\n",
    "    Добавляет префиксы к столбцам (кроме \"Номенклатура\") и формирует колонку diff_columns.\n",
    "    \"\"\"\n",
    "    if 'Номенклатура' not in df1.columns or 'Номенклатура' not in df2.columns:\n",
    "        raise ValueError(\"Оба DataFrame должны содержать колонку 'Номенклатура'\")\n",
    "\n",
    "    # Определим общие характеристики (кроме \"Номенклатура\")\n",
    "    common_columns = df1.columns.intersection(df2.columns).difference(['Номенклатура'])\n",
    "\n",
    "    # Сузим входные DataFrame'ы до нужных колонок\n",
    "    df1_reduced = df1[['Номенклатура'] + list(common_columns)].copy()\n",
    "    df2_reduced = df2[['Номенклатура'] + list(common_columns)].copy()\n",
    "\n",
    "    # Переименуем характеристики с префиксами, \"Номенклатура\" оставим без изменений\n",
    "    df1_renamed = df1_reduced.rename(columns={col: f'{name1}_{col}' for col in common_columns})\n",
    "    df2_renamed = df2_reduced.rename(columns={col: f'{name2}_{col}' for col in common_columns})\n",
    "\n",
    "    # Объединение по \"Номенклатура\"\n",
    "    df_merged = pd.merge(df1_renamed, df2_renamed, on='Номенклатура', how='outer')\n",
    "\n",
    "    # Функция для сравнения значений по строке\n",
    "    def get_differences(row):\n",
    "        diffs = []\n",
    "        for col in common_columns:\n",
    "            val1 = row.get(f'{name1}_{col}', None)\n",
    "            val2 = row.get(f'{name2}_{col}', None)\n",
    "            if pd.isna(val1) or pd.isna(val2):\n",
    "                continue\n",
    "            else:\n",
    "                tokens1 = tokenize(str(val1))\n",
    "                tokens2 = tokenize(str(val2))\n",
    "                if not are_words_possibly_synonyms(tokens1, tokens2, are_synonyms):\n",
    "                    diffs.append(col)\n",
    "        return ', '.join(diffs)\n",
    "\n",
    "    # Добавление столбца с различиями\n",
    "    df_merged['diff_columns'] = df_merged.apply(get_differences, axis=1)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "# --------------------------------------------------------------Сохранить результат сравнения в excel--------------------------------------------------------------\n",
    "\n",
    "def save_comparison_to_excel(df: pd.DataFrame, filename: str):\n",
    "    red_fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "    yellow_fill = PatternFill(start_color=\"FFEB9C\", end_color=\"FFEB9C\", fill_type=\"solid\")\n",
    "\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    # Записываем DataFrame в Excel\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws.append(r)\n",
    "\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "    diff_col_index = len(headers)\n",
    "    prefix_columns = [col for col in headers if col != 'Номенклатура' and col != 'diff_columns']\n",
    "\n",
    "    # Автонастройка ширины столбцов по первой строке\n",
    "    for col_idx, cell in enumerate(ws[1], start=1):\n",
    "        max_length = len(str(cell.value)) if cell.value else 0\n",
    "        col_letter = cell.column_letter\n",
    "        ws.column_dimensions[col_letter].width = max_length + 2  # +2 для отступа\n",
    "\n",
    "    # Применение стилей и переносов\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, max_col=ws.max_column):\n",
    "        diff_text = row[diff_col_index - 1].value\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical='center')  # включаем перенос текста\n",
    "\n",
    "        if isinstance(diff_text, str) and diff_text.strip():\n",
    "            different_fields = [field.strip() for field in diff_text.split(',')]\n",
    "            for diff_field in different_fields:\n",
    "                for col_idx, col_name in enumerate(headers):\n",
    "                    if col_name.endswith(f\"_{diff_field}\"):\n",
    "                        row[col_idx].fill = yellow_fill\n",
    "        row[diff_col_index - 1].fill = red_fill\n",
    "\n",
    "    wb.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3398e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестировать сравнение двух DataFrame и сохранение результата в excel\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(site1='korting', site2='housedorf')\n",
    "if not args.site2:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "if args.site1 == args.site2:\n",
    "    raise ValueError(\"the arguments coincide, comparison is impossible\")\n",
    "\n",
    "resultdf_1 = pd.read_parquet(f\"{args.site1}_auxiliary.parquet\")\n",
    "resultdf_2 = pd.read_parquet(f\"{args.site2}_auxiliary.parquet\")\n",
    "comp_result = compare_dataframes(resultdf_1, resultdf_2, args.site1, args.site2)\n",
    "\n",
    "save_comparison_to_excel(comp_result, f'comparison_{args.site1}_vs_{args.site2}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06585d3",
   "metadata": {},
   "source": [
    "# Программа автоматической проверки обновлений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce317a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import argparse\n",
    "from itertools import product\n",
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def tokenize(text):\n",
    "    return set(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "def are_words_possibly_synonyms(words1, words2, are_synonims_func):\n",
    "    pairs = product(words1, words2)\n",
    "    for w1, w2 in pairs:\n",
    "        if w1 == w2:\n",
    "            return True\n",
    "        if are_synonims_func(w1, w2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# -------------------------------------------------------------------Сравнить записанные данные-------------------------------------------------------------------\n",
    "\n",
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, name1: str = 'df1', name2: str = 'df2') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Сравнивает два DataFrame по общим столбцам, включая \"Номенклатура\" как ключ.\n",
    "    Добавляет префиксы к столбцам (кроме \"Номенклатура\") и формирует колонку diff_columns.\n",
    "    \"\"\"\n",
    "    if 'Номенклатура' not in df1.columns or 'Номенклатура' not in df2.columns:\n",
    "        raise ValueError(\"Оба DataFrame должны содержать колонку 'Номенклатура'\")\n",
    "\n",
    "    # Определим общие характеристики (кроме \"Номенклатура\")\n",
    "    common_columns = df1.columns.intersection(df2.columns).difference(['Номенклатура'])\n",
    "\n",
    "    # Сузим входные DataFrame'ы до нужных колонок\n",
    "    df1_reduced = df1[['Номенклатура'] + list(common_columns)].copy()\n",
    "    df2_reduced = df2[['Номенклатура'] + list(common_columns)].copy()\n",
    "\n",
    "    # Переименуем характеристики с префиксами, \"Номенклатура\" оставим без изменений\n",
    "    df1_renamed = df1_reduced.rename(columns={col: f'{name1}_{col}' for col in common_columns})\n",
    "    df2_renamed = df2_reduced.rename(columns={col: f'{name2}_{col}' for col in common_columns})\n",
    "\n",
    "    # Объединение по \"Номенклатура\"\n",
    "    df_merged = pd.merge(df1_renamed, df2_renamed, on='Номенклатура', how='outer')\n",
    "\n",
    "    # Функция для сравнения значений по строке\n",
    "    def get_differences(row):\n",
    "        diffs = []\n",
    "        for col in common_columns:\n",
    "            val1 = row.get(f'{name1}_{col}', None)\n",
    "            val2 = row.get(f'{name2}_{col}', None)\n",
    "            if pd.isna(val1) or pd.isna(val2):\n",
    "                continue\n",
    "            else:\n",
    "                tokens1 = tokenize(str(val1))\n",
    "                tokens2 = tokenize(str(val2))\n",
    "                if not are_words_possibly_synonyms(tokens1, tokens2, are_synonyms):\n",
    "                    diffs.append(col)\n",
    "        return ', '.join(diffs)\n",
    "\n",
    "    # Добавление столбца с различиями\n",
    "    df_merged['diff_columns'] = df_merged.apply(get_differences, axis=1)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "# --------------------------------------------------------------Сохранить результат сравнения в excel--------------------------------------------------------------\n",
    "\n",
    "def save_comparison_to_excel(df: pd.DataFrame, filename: str):\n",
    "    red_fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "    yellow_fill = PatternFill(start_color=\"FFEB9C\", end_color=\"FFEB9C\", fill_type=\"solid\")\n",
    "\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    # Записываем DataFrame в Excel\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws.append(r)\n",
    "\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "    diff_col_index = len(headers)\n",
    "    prefix_columns = [col for col in headers if col != 'Номенклатура' and col != 'diff_columns']\n",
    "\n",
    "    # Автонастройка ширины столбцов по первой строке\n",
    "    for col_idx, cell in enumerate(ws[1], start=1):\n",
    "        max_length = len(str(cell.value)) if cell.value else 0\n",
    "        col_letter = cell.column_letter\n",
    "        ws.column_dimensions[col_letter].width = max_length + 2  # +2 для отступа\n",
    "\n",
    "    # Применение стилей и переносов\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, max_col=ws.max_column):\n",
    "        diff_text = row[diff_col_index - 1].value\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical='center')  # включаем перенос текста\n",
    "\n",
    "        if isinstance(diff_text, str) and diff_text.strip():\n",
    "            different_fields = [field.strip() for field in diff_text.split(',')]\n",
    "            for diff_field in different_fields:\n",
    "                for col_idx, col_name in enumerate(headers):\n",
    "                    if col_name.endswith(f\"_{diff_field}\"):\n",
    "                        row[col_idx].fill = yellow_fill\n",
    "        row[diff_col_index - 1].fill = red_fill\n",
    "\n",
    "    wb.save(filename)\n",
    "\n",
    "# Протестировать сравнение двух DataFrame и сохранение результата в excel\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(site1='korting', site2='housedorf')\n",
    "if not args.site2:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "if args.site1 == args.site2:\n",
    "    raise ValueError(\"the arguments coincide, comparison is impossible\")\n",
    "\n",
    "resultdf_1 = pd.read_parquet(f\"{args.site1}_auxiliary.parquet\")\n",
    "resultdf_2 = pd.read_parquet(f\"{args.site2}_auxiliary.parquet\")\n",
    "comp_result = compare_dataframes(resultdf_1, resultdf_2, args.site1, args.site2)\n",
    "\n",
    "save_comparison_to_excel(comp_result, f'comparison_{args.site1}_vs_{args.site2}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d69d1",
   "metadata": {},
   "source": [
    "# Обработка данных для rag-агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e80119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст сохранён в hev_642_b_raw.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_clean_text(url: str, timeout: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Загружает страницу и возвращает очищенный текст без HTML.\n",
    "    Подходит для карточек товаров, статей, лендингов.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, timeout=timeout)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    html = response.text\n",
    "\n",
    "    soup_full = BeautifulSoup(html, \"lxml\")\n",
    "    soup = soup_full.body or soup_full\n",
    "\n",
    "    # 2. Удаляем мусор\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"img\", \"iframe\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 3. Получаем текст\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # 4. Нормализация\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)   # лишние переводы строк\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)      # лишние пробелы\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "url = \"https://www.mvideo.ru/products/vstraivaemaya-elektricheskaya-plita-evelux-hev-641-b-400008320/reviews?utm_source=google&utm_medium=organic&utm_campaign=google&utm_referrer=google\"\n",
    "text = extract_clean_text(url)\n",
    "\n",
    "with open(\"hev_642_b_raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"Текст сохранён в hev_642_b_raw.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "184f510a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m     29\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.mvideo.ru/products/vstraivaemaya-elektricheskaya-plita-evelux-hev-641-b-400008320/reviews?utm_source=google&utm_medium=organic&utm_campaign=google&utm_referrer=google\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_text_playwright(url)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhev_642_b_raw.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     33\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mextract_text_playwright\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_text_playwright\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m     10\u001b[0m         browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m         page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page(locale\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mru-RU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "File \u001b[1;32mc:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[0m, in \u001b[0;36mPipeTransport.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         startupinfo\u001b[38;5;241m.\u001b[39mwShowWindow \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mSW_HIDE\n\u001b[0;32m    119\u001b[0m     executable_path, entrypoint_path \u001b[38;5;241m=\u001b[39m compute_driver_executable()\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_subprocess_exec(\n\u001b[0;32m    121\u001b[0m         executable_path,\n\u001b[0;32m    122\u001b[0m         entrypoint_path,\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-driver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m         stdin\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    125\u001b[0m         stdout\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m    126\u001b[0m         stderr\u001b[38;5;241m=\u001b[39m_get_stderr_fileno(),\n\u001b[0;32m    127\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32768\u001b[39m,\n\u001b[0;32m    128\u001b[0m         env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m    129\u001b[0m         startupinfo\u001b[38;5;241m=\u001b[39mstartupinfo,\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_error_future\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\subprocess.py:218\u001b[0m, in \u001b[0;36mcreate_subprocess_exec\u001b[1;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[0;32m    215\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m    216\u001b[0m protocol_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m    217\u001b[0m                                                     loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m--> 218\u001b[0m transport, protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39msubprocess_exec(\n\u001b[0;32m    219\u001b[0m     protocol_factory,\n\u001b[0;32m    220\u001b[0m     program, \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    221\u001b[0m     stdin\u001b[38;5;241m=\u001b[39mstdin, stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[0;32m    222\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "File \u001b[1;32mc:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py:1670\u001b[0m, in \u001b[0;36mBaseEventLoop.subprocess_exec\u001b[1;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     debug_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[1;32m-> 1670\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_subprocess_transport(\n\u001b[0;32m   1671\u001b[0m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[0;32m   1672\u001b[0m     bufsize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1674\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, debug_log, transport)\n",
      "File \u001b[1;32mc:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py:498\u001b[0m, in \u001b[0;36mBaseEventLoop._make_subprocess_transport\u001b[1;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[0;32m    495\u001b[0m                                      stdin, stdout, stderr, bufsize,\n\u001b[0;32m    496\u001b[0m                                      extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "\n",
    "async def extract_text_playwright(url: str) -> str:\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page(locale=\"ru-RU\")\n",
    "\n",
    "        await page.goto(url, timeout=30000)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"img\", \"iframe\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "url = \"https://www.mvideo.ru/products/vstraivaemaya-elektricheskaya-plita-evelux-hev-641-b-400008320/reviews?utm_source=google&utm_medium=organic&utm_campaign=google&utm_referrer=google\"\n",
    "text = await extract_text_playwright(url)\n",
    "\n",
    "with open(\"hev_642_b_raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"Текст сохранён в hev_642_b_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33b0a72a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (938024582.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    playwright install\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ee61a",
   "metadata": {},
   "source": [
    "# Программа processing_similar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c2bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import re\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e4b8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составить Пары совпадающих по названиям столбцов в файле result.xlsx\n",
    "\n",
    "def find_duplicate_column_pairs(file_path: str):\n",
    "    \"\"\"\n",
    "    Находит пары совпадающих по названию столбцов в Excel-файле.\n",
    "    \n",
    "    :param file_path: путь к файлу Excel\n",
    "    :return: список пар (col_name, index1, index2)\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    col_names = list(df.columns)\n",
    "    print(col_names)\n",
    "\n",
    "    # убираем '.1', '.2', ...\n",
    "    norm_cols = []\n",
    "    for c in col_names:\n",
    "        s = str(c).strip()\n",
    "        s = re.sub(r\"\\.\\d+$\", \"\", s)  # убираем '.1', '.2', ...\n",
    "        norm_cols.append(s)\n",
    "\n",
    "    pairs = []\n",
    "    for col in set(norm_cols):\n",
    "        # если имя встречается более одного раза\n",
    "        indices = [i for i, name in enumerate(norm_cols) if name == col]\n",
    "        if len(indices) > 1:\n",
    "            # составляем все возможные пары\n",
    "            for i1, i2 in combinations(indices, 2):\n",
    "                pairs.append((col, i1, i2))\n",
    "    \n",
    "    return pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08aa265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Номенклатура', '^Циф. Арт.', '^Бук. Арт.', '*ЦЕННИК МЕБЕЛЬЩИК', 'EASY INSTALLATION', 'АВТОМАТИКА ЗАКИПАНИЯ', 'АВТОМАТИКА ЗАКИПАНИЯ.1', 'АВТОМАТИЧЕСКИЙ РЕЖИМ', 'АВТОМАТИЧЕСКИЙ РЕЖИМ.1', 'АВТОМАТИЧЕСКИЙ РЕЖИМ (СКОРОСТЬ ВСАСЫВАНИЯ АВТОМАТИЧЕСКИ УСТАНАВЛИВАЕТСЯ ПРИ ПРИГОТОВЛЕНИИ ПИЩИ)', 'АВТОМАТИЧЕСКОЕ ОПРЕДЕЛЕНИЕ ДИАМЕТРА ПОСУДЫ', 'АВТОМАТИЧЕСКОЕ ОПРЕДЕЛЕНИЕ НАЛИЧИЯ ПОСУДЫ', 'АВТОМАТИЧЕСКОЕ РАСПОЗНАВАНИЕ ПОСУДЫ', 'АВТООТКРЫВАНИЕ ДВЕРЦЫ', 'АСПИРАЦИЯ ПО ПЕРИМЕТРУ', 'БЕЗОПАСНОЕ ВЫКЛЮЧЕНИЕ', 'БЕСШУМНАЯ РАБОТА', 'БЛОКИРОВКА КНОПОК', 'БЛОКИРОВКА ОТ ДЕТЕЙ', 'БЛОКИРОВКА ПАНЕЛИ УПРАВЛЕНИЯ (ЗАЩИТА ОТ ДЕТЕЙ)', 'БЛОКИРОВКА УПРАВЛЕНИЯ/ЗАЩИТА ОТ ДЕТЕЙ', 'БРЕНД', 'БЫСТРЫЙ НАГРЕВ ДУХОВКИ', 'БЫСТРЫЙ ПРЕДВАРИТЕЛЬНЫЙ РАЗОГРЕВ', 'варочная', 'ВЕС (КГ)', 'ВЕС БЕЗ УПАКОВКИ, КГ', 'ВЕС БРУТТО (КГ)', 'ВЕС НЕТТО (КГ)', 'ВЕС С УПАКОВКОЙ, КГ', 'ВИД', 'ВИД.1', 'ВИД ДУХОВОГО ШКАФА', 'ВИДЫ НАГРЕВА', 'ВМЕСТИМОСТЬ (БУТЫЛКИ 0.75 Л)', 'ВНУТРЕННЕЕ ОСВЕЩЕНИЕ', 'ВОЗМОЖНОСТЬ ПРОГРАММИРОВАНИЯ', 'ВОЗМОЖНОСТЬ УСТАНОВКИ АКТИВНОГО ФИЛЬТРА', 'ВОЗМОЖНОСТЬ УСТАНОВКИ ПОДВЕСНОЙ ВЫТЯЖКИ НА СТЕНУ', 'ВОЗМОЖНОСТЬ УСТАНОВКИ УГОЛЬНОГО ФИЛЬТРА', 'ВОЗМОЖНОСТЬ УСТАНОВКИ УГОЛЬНОГО ФИЛЬТРА.1', 'ВСТРОЕННАЯ ВЫТЯЖКА', 'ВЫВОД ВОЗДУХА', 'ВЫДВИЖНАЯ ПАНЕЛЬ', 'ВЫСОКИЕ ЭКСПЛУАТАЦИОННЫЕ ХАРАКТЕРИСТИКИ HIGH PERFOMANCE', 'ВЫСОТА (СМ)', 'ВЫСОТА КБТ (условная)', 'ВЫТЯЖКА', 'ГАБАРИТЫ (ВХШХГ) (СМ)', 'ГАБАРИТЫ (ВХШХГ), СМ', 'ГАБАРИТЫ НИШИ ДЛЯ ВСТРАИВАНИЯ (ВХШХГ), СМ', 'ГАБАРИТЫ, ВХШХГ (СМ)', 'ГЛУБИНА (СМ)', 'ГЛУБИНА ВСТРАИВАНИЯ (СМ)', 'ГЛУБИНА КБТ (условная)', 'ГОДОВОЕ ПОТРЕБЛЕНИЕ ЭНЕРГИИ (КВТЧ/ГОД)', 'ГОДОВОЕ ЭНЕРГОПОТРЕБЛЕНИЕ, КВТ', 'ГОЛОСОВОЕ УПРАВЛЕНИЕ', 'ГРИЛЬ', 'ГРИЛЬ.1', 'ГРИЛЬ + КОНВЕКЦИЯ', 'ДИАМЕТР ВОЗДУХОВОДА (ММ)', 'ДИАМЕТР ВОЗДУХОВОДА (ММ).1', 'ДИАМЕТР ВОЗДУХОВОДА, ММ', 'ДИАМЕТР ЛЕВОЙ БЛИЖНЕЙ КОНФОРКИ, СМ', 'ДИАМЕТР ЛЕВОЙ ДАЛЬНЕЙ КОНФОРКИ, СМ', 'ДИАМЕТР ЛЕВОЙ КОНФОРКИ, СМ', 'ДИАМЕТР ОБЪЕДИНЯЕМОЙ ЗОНЫ, СМ', 'ДИАМЕТР ПРАВОЙ БЛИЖНЕЙ КОНФОРКИ, СМ', 'ДИАМЕТР ПРАВОЙ ДАЛЬНЕЙ КОНФОРКИ, СМ', 'ДИАМЕТР ПРАВОЙ КОНФОРКИ, СМ', 'ДИАПАЗОН ТЕМПЕРАТУР (°C)', 'ДИАПАЗОН УСТАНОВКИ ТЕМПЕРАТУРЫ (°C)', 'ДИЗАЙН', 'ДИЗАЙН ВЫТЯЖКИ', 'ДИСПЛЕЙ', 'ДИСПЛЕЙ.1', 'ДИСПЛЕЙ.2', 'ДЛЯ ВСТРАИВАНИЯ В ШКАФ', 'ДОП.ФУНКЦИИ (Варочная панель)', 'ДОПОЛНИТЕЛЬНЫЕ АКСЕССУАРЫ (ПРИОБРЕТАЮТСЯ ОТДЕЛЬНО)', 'ДОПОЛНИТЕЛЬНЫЕ ВОЗМОЖНОСТИ', 'ДОПОЛНИТЕЛЬНЫЕ ПАРАМЕТРЫ', 'Духовой шкаф', 'ЕМКОСТЬ ВИННОЙ КАМЕРЫ, БУТЫЛОК', 'ЖИРОВОЙ ФИЛЬТР (ШТ)', 'ЖИРОУЛАВЛИВАЮЩИЙ ФИЛЬТР', 'ЖИРОУЛАВЛИВАЮЩИЙ ФИЛЬТР ПРИГОДЕН ДЛЯ МЫТЬЯ В ПОСУДОМОЕЧНОЙ МАШИНЕ', 'ЗАДНЯЯ ЛЕВАЯ (КВТ)', 'ЗАДНЯЯ ЛЕВАЯ (КВТ).1', 'ЗАДНЯЯ ЛЕВАЯ (ММ)', 'ЗАДНЯЯ ЛЕВАЯ (ММ).1', 'ЗАДНЯЯ ПРАВАЯ (КВТ)', 'ЗАДНЯЯ ПРАВАЯ (КВТ).1', 'ЗАДНЯЯ ПРАВАЯ (ММ)', 'ЗАДНЯЯ ПРАВАЯ (ММ).1', 'ЗАДНЯЯ ЦЕНТРАЛЬНАЯ (КВТ)', 'ЗАДНЯЯ ЦЕНТРАЛЬНАЯ (КВТ).1', 'ЗАДНЯЯ ЦЕНТРАЛЬНАЯ (ММ)', 'ЗАДНЯЯ ЦЕНТРАЛЬНАЯ (ММ).1', 'ЗАКУП!***', 'ЗАЩИТА ОТ УФ-ИЗЛУЧЕНИЯ / ТОНИРОВКА', 'ЗОНЫ BRIDGE', 'ЗОНЫ НАГРЕВА', 'ИНДИКАТОР ЗАГРЯЗНЕНИЯ ФИЛЬТРА', 'ИНДИКАТОР ЗАГРЯЗНЕНИЯ ФИЛЬТРА.1', 'ИНДИКАТОРЫ', 'ИНДИКАЦИЯ ЗАГРЯЗНЕНИЯ ФИЛЬТРОВ', 'ИНДИКАЦИЯ НАСЫЩЕНИЯ ФИЛЬТРА', 'ИНДИКАЦИЯ ОСТАТОЧНОГО ТЕПЛА', 'ИНДИКАЦИЯ СТУПЕНЕЙ МОЩНОСТИ', 'ИНТЕНСИВНЫЙ РЕЖИМ', 'ИНТЕНСИВНЫЙ РЕЖИМ.1', 'ИНТЕНСИВНЫЙ РЕЖИМ.2', 'КАТЕГОРИЯ (мбт, кбт, сантехника, столеш и тд)', 'КАТЕГОРИЯ ТОВАРА ВНУТРИ БРЕНДА', 'КЛАСС ЭНЕРГОПОТРЕБЛЕНИЯ', 'КЛАСС ЭНЕРГОПОТРЕБЛЕНИЯ.1', 'КЛАСС ЭНЕРГОПОТРЕБЛЕНИЯ ВЫТЯЖКИ', 'КЛАСС ЭНЕРГОЭФФЕКТИВНОСТИ', 'КЛИМАТИЧЕСКИЙ КЛАСС', 'КОЛИЧЕСТВО АВТОМАТИЧЕСКИХ ПРОГРАММ', 'КОЛИЧЕСТВО АВТОМАТИЧЕСКИХ ПРОГРАММ.1', 'КОЛИЧЕСТВО ДВЕРЕЙ', 'КОЛИЧЕСТВО ДВИГАТЕЛЕЙ', 'КОЛИЧЕСТВО ЗОН НАГРЕВА', 'КОЛИЧЕСТВО ЗОН ПРИГОТОВЛЕНИЯ', 'КОЛИЧЕСТВО КАМЕР', 'КОЛИЧЕСТВО КОНФОРОК', 'КОЛИЧЕСТВО ЛАМП', 'КОЛИЧЕСТВО ЛАМП ОСВЕЩЕНИЯ', 'КОЛИЧЕСТВО ОБЪЕДИНЯЕМЫХ ЗОН', 'КОЛИЧЕСТВО ПОЛОК В ВИННОМ ОТДЕЛЕНИИ', 'КОЛИЧЕСТВО СКОРОСТЕЙ', 'КОЛИЧЕСТВО СКОРОСТЕЙ.1', 'КОЛИЧЕСТВО СПЕЦИАЛЬНЫХ ПРОГРАММ', 'КОЛИЧЕСТВО СПЕЦИАЛЬНЫХ ПРОГРАММ.1', 'КОЛИЧЕСТВО СТЕКОЛ ДВЕРЦЫ ДУХОВКИ', 'КОЛИЧЕСТВО УРОВНЕЙ МОЩНОСТИ', 'КОЛИЧЕСТВО УРОВНЕЙ МОЩНОСТИ ВЫТЯЖКИ', 'КОЛИЧЕСТВО УРОВНЕЙ МОЩНОСТИ ВЫТЯЖКИ.1', 'КОЛИЧЕСТВО ФИЛЬТРОВ', 'КОЛИЧЕСТВО ФИЛЬТРОВ.1', 'КОМПЛЕКТАЦИЯ', 'КОМПЛЕКТАЦИЯ.1', 'КОНВЕКЦИЯ', 'КОНВЕКЦИЯ.1', 'КОНФОРОК ГАЗОВЫХ', 'КОНФОРОК ДВОЙНАЯ КОРОНА', 'КОНФОРОК ИНДУКЦИОННЫХ', 'МАКС. ПРОИЗВОДИТЕЛЬНОСТЬ (М3/Ч)', 'МАКСИМАЛЬНАЯ ВЫСОТА (СМ)', 'МАКСИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ', 'МАКСИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ (М3/Ч)', 'МАКСИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ В РЕЖИМЕ ОТВОДА, М³/Ч', 'МАКСИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ В РЕЖИМЕ РЕЦИРКУЛЯЦИИ, М³/Ч', 'МАКСИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ М3/ЧАС', 'МАКСИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ, М³/Ч', 'МАКСИМАЛЬНЫЙ УРОВЕНЬ ШУМА (ДБ)', 'МАКСИМАЛЬНЫЙ УРОВЕНЬ ШУМА, ДБ', 'МАТЕРИАЛ', 'МАТЕРИАЛ ВАРОЧНОЙ ПАНЕЛИ', 'МАТЕРИАЛ ПОЛОК', 'МАТЕРИАЛ ПОЛОК В ВИННОЙ КАМЕРЕ', 'МЕДЛЕННОЕ ПРИГОТОВЛЕНИЕ', 'МЕДЛЕННОЕ ПРИГОТОВЛЕНИЕ.1', 'МИНИМАЛЬНЫЙ УРОВЕНЬ ШУМА (ДБ)', 'МИНИМАЛЬНЫЙ УРОВЕНЬ ШУМА, ДБ', 'МОНТАЖ', 'МОНТАЖНЫЙ КОМПЛЕКТ ДЛЯ РАБОТЫ В РЕЖИМЕ ОТВОДА ПРИОБРЕТАЕТСЯ ОТДЕЛЬНО', 'МОЩНОСТЬ ВЫТЯЖКИ (ВТ)', 'МОЩНОСТЬ КАЖДОЙ ЛАМПЫ (ВТ)', 'МОЩНОСТЬ КАЖДОЙ ЛАМПЫ, ВТ', 'МОЩНОСТЬ ЛЕВОЙ БЛИЖНЕЙ КОНФОРКИ, КВТ', 'МОЩНОСТЬ ЛЕВОЙ ДАЛЬНЕЙ КОНФОРКИ, КВТ', 'МОЩНОСТЬ ОБЪЕДИНЯЕМОЙ ЗОНЫ, КВТ', 'МОЩНОСТЬ ПРАВОЙ БЛИЖНЕЙ КОНФОРКИ, КВТ', 'МОЩНОСТЬ ПРАВОЙ ДАЛЬНЕЙ КОНФОРКИ, КВТ', 'НАГРЕВ (варочная панель)', 'НАИМЕНОВАНИЕ АКСЕССУАРА', 'НАИМЕНОВАНИЕ ТОВАРА', 'НАПРАВЛЯЮЩИЕ', 'НАПРЯЖЕНИЕ (В)', 'НАПРЯЖЕНИЕ (В).1', 'НАПРЯЖЕНИЕ (В).2', 'НАПРЯЖЕНИЕ ЭЛЕКТРОПИТАНИЯ (В)', 'НИЖНИЙ НАГРЕВ', 'НИЗКОТЕМПЕРАТУРНОЕ ПРИГОТОВЛЕНИЕ', 'НОМИНАЛЬНАЯ МОЩНОСТЬ (КВТ)', 'НОМИНАЛЬНАЯ МОЩНОСТЬ, ВТ', 'НОМИНАЛЬНАЯ МОЩНОСТЬ, КВТ', 'ОБОРУДОВАНИЕ ДЛЯ РАБОТЫ В РЕЖИМЕ РЕЦИРКУЛЯЦИИ ПРИОБРЕТАЕТСЯ ОТДЕЛЬНО', 'ОБРАБОТКА КРАЯ', 'ОБЩАЯ МОЩНОСТЬ КОНФОРОК (КВТ)', 'ОБЩАЯ ПОТРЕБЛЯЕМАЯ МОЩНОСТЬ (КВТ)', 'ОБЩЕЕ КОЛИЧЕСТВО КОНФОРОК', 'ОБЩЕЕ КОЛИЧЕСТВО ПОЛОК', 'ОБЩИЙ ОБЪЕМ (Л)', 'ОБЪЕМ (Л)', 'ОБЪЕМ РЕЗЕРВУАРА ДЛЯ ВОДЫ (Л)', 'ОПРЕДЕЛЕНИЕ НАЛИЧИЯ ПОСУДЫ', 'ОСВЕЩЕНИЕ ДУХОВКИ', 'ОСВЕЩЕНИЕ РАБОЧЕГО МЕСТА', 'ОСОБЕННОСТИ КОНСТРУКЦИИ', 'ОСОБЕННОСТИ МОДЕЛИ', 'ОСОБЕННОСТИ ПРИБОРА', 'ОСОБЕННОСТИ УПРАВЛЕНИЯ', 'ОСОБЕННОСТИ ФУНКЦИОНАЛА', 'ОТКЛЮЧЕНИЕ', 'ОТКРЫВАНИЕ (винный шкаф)', 'ОЧИСТКА (ДШ и варочных центров)', 'ПАНЕЛЬ КОНФОРОК', 'ПАНЕЛЬ УПРАВЛЕНИЯ И ДИСПЛЕЙ', 'ПАР', 'ПЕРЕДНЯЯ ЛЕВАЯ (КВТ)', 'ПЕРЕДНЯЯ ЛЕВАЯ (КВТ).1', 'ПЕРЕДНЯЯ ЛЕВАЯ (ММ)', 'ПЕРЕДНЯЯ ЛЕВАЯ (ММ).1', 'ПЕРЕДНЯЯ ПРАВАЯ (КВТ)', 'ПЕРЕДНЯЯ ПРАВАЯ (КВТ).1', 'ПЕРЕДНЯЯ ПРАВАЯ (ММ)', 'ПЕРЕДНЯЯ ПРАВАЯ (ММ).1', 'ПЕРЕКЛЮЧАТЕЛИ', 'ПЕРЕКЛЮЧАТЕЛИ / ПАНЕЛЬ УПРАВЛЕНИЯ', 'ПИЦЦА', 'ПИЦЦА.1', 'ПЛАВНОЕ ЗАКРЫТИЕ ДВЕРИ SOFTCLOSING', 'ПЛАСТИНА ПОВЕРХ ЖИРОУЛАВЛИВАЮЩЕГО ФИЛЬТРА', 'ПОДДЕРЖАНИЕ ТЕМПЕРАТУРЫ', 'ПОДКАТЕГОРИЯ 1', 'ПОДРАЗДЕЛ КБТ', 'ПОЛЕЗНЫЙ ОБЪЕМ (Л)', 'ПОТРЕБЛЯЕМАЯ МОЩНОСТЬ (ВТ)', 'ПОТРЕБЛЯЕМАЯ МОЩНОСТЬ ВАРОЧНОЙ ПАНЕЛИ, КВТ', 'ПОТРЕБЛЯЕМАЯ МОЩНОСТЬ ВЫТЯЖКИ, ВТ', 'ПРИОБРЕТАЕТСЯ ОТДЕЛЬНО', 'ПРОГРАММИРОВАНИЕ TEMPERATURE MANAGER', 'ПРОГРАММЫ ПРИГОТОВЛЕНИЯ НА ПАРУ, ШТ.', 'ПРОИЗВОДИТЕЛЬНОСТЬ ВЫТЯЖКИ', 'ПРОИЗВОДИТЕЛЬНОСТЬ НА ВТОРОЙ СТУПЕНИ (М3/Ч)', 'ПРОИЗВОДИТЕЛЬНОСТЬ НА ИНТЕНСИВНОЙ СТУПЕНИ (М3/Ч)', 'ПРОИЗВОДИТЕЛЬНОСТЬ НА ПЕРВОЙ СТУПЕНИ (М3/Ч)', 'ПРОИЗВОДИТЕЛЬНОСТЬ, М3/Ч', 'ПРОСТАЯ УСТАНОВКА EASY INSTALLATION', 'ПУЛЬТ ДИСТАНЦИОННОГО УПРАВЛЕНИЯ', 'РАБОТА В РЕЖИМАХ', 'РАЗДЕЛ КБТ', 'РАЗМЕР НИШИ ДЛЯ ВСТРАИВАНИЯ (ШХГ), СМ', 'РАЗМЕРЫ В РЕЖИМЕ ОТВОДА ВОЗДУХА (ВХШХГ)', 'РАЗМЕРЫ НИШИ ДЛЯ ВСТРАИВАНИЯ (Ш Х Г), СМ', 'РАСПОЛОЖЕНИЕ', 'РАСПОЛОЖЕНИЕ ЭЛЕМЕНТОВ УПРАВЛЕНИЯ', 'РЕГЕНЕРАЦИЯ', 'РЕГУЛИРОВКА БЕЛОГО ТОНА ОСВЕЩЕНИЯ TUNE WHITE', 'РЕГУЛИРОВКА МОЩНОСТИ', 'РЕЖИМ POWER', 'РЕЖИМЫ РАБОТЫ', 'РЕЖИМЫ РАБОТЫ.1', 'РЕЖИМЫ РАБОТЫ.2', 'РЕШЕТКА', 'РЕШЕТКА.1', 'СЕРИЙНАЯ КОМПЛЕКТАЦИЯ', 'СЕРИЯ', 'СИГНАЛИЗАЦИЯ НЕИСПРАВНОСТЕЙ', 'СОВМЕСТИМОСТЬ РАБОТЫ С УСТРОЙСТВОМ SNAP', 'СОВМЕСТИМОСТЬ С УСТРОЙСТВОМ SNAP', 'СПОСОБ ПОДКЛЮЧЕНИЯ', 'СРОК ГАРАНТИИ', 'СТАТУС', 'СТЕКЛЯННЫЙ ПРОТИВЕНЬ', 'СТЕКЛЯННЫЙ ЭКРАН', 'СТИЛЬ', 'СТРАНА ПРОИЗВОДСТВА', 'СТУПЕНЕЙ МОЩНОСТИ НАГРЕВА', 'ТАЙМЕР', 'ТАЙМЕР.1', 'ТЕМПЕРАТУРНЫЕ ЗОНЫ', 'ТЕМПЕРАТУРНЫЕ ЗОНЫ (винный шкаф)', 'ТЕРМОЩУП', 'ТИП ВАРОЧНОЙ ПОВЕРХНОСТИ', 'ТИП ВСТРАИВАНИЯ', 'ТИП ВЫТЯЖКИ', 'ТИП ОСВЕЩЕНИЯ', 'ТИП ОХЛАЖДЕНИЯ', 'ТИП ОХЛАЖДЕНИЯ.1', 'ТИП ТАЙМЕРА', 'ТИП УГОЛЬНОГО ФИЛЬТРА', 'ТИП УПРАВЛЕНИЯ', 'ТИП УСТАНОВКИ (КБТ)', 'ТИХАЯ РАБОТА', 'УГОЛЬНЫЙ ФИЛЬТР', 'УПРАВЛЕНИЕ', 'УПРАВЛЕНИЕ.1', 'УПРАВЛЕНИЕ (варочная панель)', 'УПРАВЛЕНИЕ ЧЕРЕЗ ПРИЛОЖЕНИЕ ELICACONNECT', 'УРОВЕНЬ ШУМА (ДБ)', 'УРОВЕНЬ ШУМА В ИНТЕНСИВНОМ РЕЖИМЕ (ДБ)', 'УРОВЕНЬ ШУМА В РЕЖИМЕ ОТВОДА, ДБ', 'УРОВЕНЬ ШУМА В РЕЖИМЕ РЕЦИРКУЛЯЦИИ, ДБ', 'УРОВЕНЬ ШУМА НА ВТОРОЙ СКОРОСТИ (ДБ)', 'УРОВЕНЬ ШУМА НА МАКСИМАЛЬНОЙ СКОРОСТИ (ДБ)', 'УРОВЕНЬ ШУМА НА ПЕРВОЙ СКОРОСТИ (ДБ)', 'УСТАНОВКА', 'УСТАНОВКА ВЫТЯЖКИ', 'ФИЛЬТР', 'ФИЛЬТР.1', 'ФИЛЬТР ПРИГОДЕН ДЛЯ МЫТЬЯ В ПОСУДОМОЕЧНОЙ МАШИНЕ', 'ФИЛЬТР С ПРОДОЛЖИТЕЛЬНЫМ СРОКОМ СЛУЖБЫ', 'ФУНКЦИЯ BOOSTER', 'ФУНКЦИЯ BRIDGE', 'ФУНКЦИЯ POWER LIMITATION (ОГРАНИЧЕНИЕ МОЩНОСТИ)', 'ФУНКЦИЯ STOP&GO', 'ФУНКЦИЯ ОБЪЕДИНЕНИЯ КОНФОРОК', 'ФУНКЦИЯ ОПРЕДЕЛЕНИЯ ДИАМЕТРА ПОСУДЫ', 'ФУНКЦИЯ ПАУЗА', 'ФУНКЦИЯ ПАУЗЫ', 'ФУНКЦИЯ ПОДДЕРЖАНИЯ ТЕПЛА', 'ФУРНИТУРА', 'ХЛАДАГЕНТ', 'ХЛЕБ', 'ХОЛОДНАЯ ДВЕРЦА (COOLDOOR)', 'ХОЛОДНАЯ ЗОНА', 'ЦВЕТ', 'ЦВЕТ.1', 'ЦВЕТ ОКАНТОВКИ', 'ЦВЕТ ПО КАТАЛОГУ (для характеристик на сайте)', 'ЦВЕТ РАМКИ', 'ЦВЕТ ФИЛЬТР (для сайта, условный)', 'ЦВЕТОВАЯ ТЕМПЕРАТУРА, К', 'ЧАСТОТА', 'ЧАСТОТА (ГЦ)', 'ЧАСЫ', 'ШИРИНА (СМ)', 'ШИРИНА ВСТРАИВАНИЯ (СМ)', 'ШИРИНА КБТ (условная)', 'ЭЛЕКТРОННЫЙ КОНТРОЛЬ ТЕМПЕРАТУРЫ', 'ЭЛЕМЕНТЫ УПРАВЛЕНИЯ', 'ЭЛЕМЕНТЫ УПРАВЛЕНИЯ.1', 'АРТИКУЛ', 'ТИП УСТАНОВКИ', 'ОСТАТОЧНЫЙ ХОД', 'ПРЕДНАЗНАЧЕНО ДЛЯ', 'ПРЕДНАЗНАЧЕНИЕ', 'КОНФОРОК ДВУХКОНТУРНЫХ', 'БЫСТРЫЙ РАЗОГРЕВ', 'ЛЕВАЯ (КВТ)', 'ЛЕВАЯ (ММ)', 'МИНИМАЛЬНАЯ ПРОИЗВОДИТЕЛЬНОСТЬ (М3/Ч)', 'ПРОИЗВОДИТЕЛЬНОСТЬ В ИНТЕНСИВНОМ РЕЖИМЕ (М3/Ч)', 'МОЩНОСТЬ ПОДКЛЮЧЕНИЯ (ВТ)', 'ГЛУБИНА (СМ)_Глубина', 'ГЛУБИНА, ММ_Глубина', 'ГЛУБИНА, СМ_Глубина', 'ВЕС (С УПАКОВКОЙ)_Вес брутто', 'ВЕС БЕЗ УПАКОВКИ/В УПАКОВКЕ, КГ_Вес брутто', 'ВЕС БРУТТО (КГ)_Вес брутто', 'ВЕС БРУТТО / НЕТТО, КГ_Вес брутто', 'ВЕС БРУТТО, КГ_Вес брутто', 'ВЕС НЕТТО/БРУТТО, КГ_Вес брутто', 'ВЕС С/БЕЗ УПАКОВКИ, КГ_Вес брутто', 'ОБЩИЙ ОБЪЕМ МОРОЗИЛЬНОЙ КАМЕРЫ (Л)_Объем камеры', 'ОБЩИЙ ПОЛЕЗНЫЙ ОБЪЕМ (Л)_Объем камеры', 'ОБЩИЙ ПОЛЕЗНЫЙ ОБЪЕМ, Л_Объем камеры', 'ОБЪЕМ ВАКУУМНОЙ КАМЕРЫ, Л_Объем камеры', 'ОБЪЕМ ВИННОЙ КАМЕРЫ, Л_Объем камеры', 'ОБЪЕМ МОРОЗИЛЬНОГО ОТДЕЛЕНИЯ, Л_Объем камеры', 'ОБЪЕМ МОРОЗИЛЬНОЙ КАМЕРЫ_Объем камеры', 'ОБЪЕМ МОРОЗИЛЬНОЙ КАМЕРЫ, Л_Объем камеры', 'ОБЪЕМ ХОЛОДИЛЬНОГО ОТДЕЛЕНИЯ, Л_Объем камеры', 'ОБЪЕМ ХОЛОДИЛЬНОЙ КАМЕРЫ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ (Л)_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ ВИННОЙ КАМЕРЫ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ ЗОНЫ СВЕЖЕСТИ (Л)_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ КАМЕРЫ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ МОРОЗИЛЬНОГО ОТДЕЛЕНИЯ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ МОРОЗИЛЬНОЙ КАМЕРЫ (Л)_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ МОРОЗИЛЬНОЙ КАМЕРЫ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ ХОЛОДИЛЬНОГО ОТДЕЛЕНИЯ, Л_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ ХОЛОДИЛЬНОЙ КАМЕРЫ (Л)_Объем камеры', 'ПОЛЕЗНЫЙ ОБЪЕМ ХОЛОДИЛЬНОЙ КАМЕРЫ, Л_Объем камеры', 'МАКСИМАЛЬНАЯ МОЩНОСТЬ МИКРОВОЛН (ВТ)_Максимальная мощность СВЧ']\n",
      "('РЕШЕТКА', 259, 260)\n",
      "('ТИП ОХЛАЖДЕНИЯ', 283, 284)\n",
      "('КОЛИЧЕСТВО СПЕЦИАЛЬНЫХ ПРОГРАММ', 135, 136)\n",
      "('АВТОМАТИЧЕСКИЙ РЕЖИМ', 7, 8)\n",
      "('ПЕРЕДНЯЯ ЛЕВАЯ (КВТ)', 214, 215)\n",
      "('ГРИЛЬ', 58, 59)\n",
      "('ВОЗМОЖНОСТЬ УСТАНОВКИ УГОЛЬНОГО ФИЛЬТРА', 39, 40)\n",
      "('ЗАДНЯЯ ПРАВАЯ (КВТ)', 92, 93)\n",
      "('ВИД', 30, 31)\n",
      "('КОЛИЧЕСТВО ФИЛЬТРОВ', 141, 142)\n",
      "('АВТОМАТИКА ЗАКИПАНИЯ', 5, 6)\n",
      "('ЗАДНЯЯ ЛЕВАЯ (КВТ)', 88, 89)\n",
      "('КОНВЕКЦИЯ', 145, 146)\n",
      "('ФИЛЬТР', 304, 305)\n",
      "('МЕДЛЕННОЕ ПРИГОТОВЛЕНИЕ', 164, 165)\n",
      "('ПИЦЦА', 224, 225)\n",
      "('КЛАСС ЭНЕРГОПОТРЕБЛЕНИЯ', 116, 117)\n",
      "('НАПРЯЖЕНИЕ (В)', 182, 183)\n",
      "('НАПРЯЖЕНИЕ (В)', 182, 184)\n",
      "('НАПРЯЖЕНИЕ (В)', 183, 184)\n",
      "('КОЛИЧЕСТВО СКОРОСТЕЙ', 133, 134)\n",
      "('ПЕРЕДНЯЯ ПРАВАЯ (ММ)', 220, 221)\n",
      "('УПРАВЛЕНИЕ', 291, 292)\n",
      "('ЭЛЕМЕНТЫ УПРАВЛЕНИЯ', 336, 337)\n",
      "('ЗАДНЯЯ ПРАВАЯ (ММ)', 94, 95)\n",
      "('ЦВЕТ', 322, 323)\n",
      "('КОМПЛЕКТАЦИЯ', 143, 144)\n",
      "('ЗАДНЯЯ ЛЕВАЯ (ММ)', 90, 91)\n",
      "('ПЕРЕДНЯЯ ПРАВАЯ (КВТ)', 218, 219)\n",
      "('КОЛИЧЕСТВО АВТОМАТИЧЕСКИХ ПРОГРАММ', 121, 122)\n",
      "('КОЛИЧЕСТВО УРОВНЕЙ МОЩНОСТИ ВЫТЯЖКИ', 139, 140)\n",
      "('ИНДИКАТОР ЗАГРЯЗНЕНИЯ ФИЛЬТРА', 104, 105)\n",
      "('ЗАДНЯЯ ЦЕНТРАЛЬНАЯ (ММ)', 98, 99)\n",
      "('ПЕРЕДНЯЯ ЛЕВАЯ (ММ)', 216, 217)\n",
      "('РЕЖИМЫ РАБОТЫ', 256, 257)\n",
      "('РЕЖИМЫ РАБОТЫ', 256, 258)\n",
      "('РЕЖИМЫ РАБОТЫ', 257, 258)\n",
      "('ИНТЕНСИВНЫЙ РЕЖИМ', 111, 112)\n",
      "('ИНТЕНСИВНЫЙ РЕЖИМ', 111, 113)\n",
      "('ИНТЕНСИВНЫЙ РЕЖИМ', 112, 113)\n",
      "('ЗАДНЯЯ ЦЕНТРАЛЬНАЯ (КВТ)', 96, 97)\n",
      "('ДИАМЕТР ВОЗДУХОВОДА (ММ)', 61, 62)\n",
      "('ДИСПЛЕЙ', 75, 76)\n",
      "('ДИСПЛЕЙ', 75, 77)\n",
      "('ДИСПЛЕЙ', 76, 77)\n",
      "('ТАЙМЕР', 274, 275)\n"
     ]
    }
   ],
   "source": [
    "# Протестировать составление совпадений\n",
    "pairs = find_duplicate_column_pairs(\"result.xlsx\")\n",
    "for p in pairs:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91bf137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составить Пары столбцов с _ и без, совпадающих по тексту в верхнем регистре\n",
    "def find_prefix_column_pairs(file_path: str):\n",
    "    \"\"\"\n",
    "    Находит пары столбцов, совпадающих по префиксу до '_'.\n",
    "    В паре всегда один столбец без '_' и один с '_'.\n",
    "    \n",
    "    :param file_path: путь к Excel файлу\n",
    "    :return: список пар (base_col, extended_col)\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    col_names = list(df.columns)\n",
    "\n",
    "    base_cols = [c for c in col_names if \"_\" not in c]\n",
    "    extended_cols = [c for c in col_names if \"_\" in c]\n",
    "\n",
    "    pairs = []\n",
    "    for ext in extended_cols:\n",
    "        prefix = ext.split(\"_\", 1)[0]  # часть до _\n",
    "        if prefix in base_cols:\n",
    "            pairs.append((prefix, ext))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39db36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ГЛУБИНА (СМ)', 'ГЛУБИНА (СМ)_Глубина')\n",
      "('ВЕС БРУТТО (КГ)', 'ВЕС БРУТТО (КГ)_Вес брутто')\n",
      "('ПОЛЕЗНЫЙ ОБЪЕМ (Л)', 'ПОЛЕЗНЫЙ ОБЪЕМ (Л)_Объем камеры')\n"
     ]
    }
   ],
   "source": [
    "# Протестировать составление аналогий\n",
    "pairs = find_prefix_column_pairs(\"result.xlsx\")\n",
    "for p in pairs:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb8f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.3 x 59.9 x 45 -> ^\\d+(?:\\.\\d)?x\\d+(?:\\.\\d)?x\\d+(?:\\.\\d)?$\n",
      "True\n",
      "24.5x59x60.5 -> ^\\d+(?:\\.\\d)?x\\d+(?:\\.\\d)?x\\d+(?:\\.\\d)?$\n",
      "False\n",
      "240x320 -> ^\\d+(?:\\.\\d)?x\\d+(?:\\.\\d)?$\n",
      "False\n",
      "240x320 см -> ^\\d+(?:\\.\\d)?x\\d+(?:\\.\\d)?см$\n"
     ]
    }
   ],
   "source": [
    "# Написать вспомогательную функцию, которая будет из строки делать регулярное выражение\n",
    "\n",
    "def make_regex_from_string(s: str, spaces=True) -> str:\n",
    "    # убираем пробелы полностью — чтобы они не влияли на разбиение (или нет)\n",
    "    if not spaces:\n",
    "        s = s.replace(\" \", \"\")\n",
    "\n",
    "    # Разбиваем строку на числа и разделители\n",
    "    tokens = re.split(r'(\\d+(?:\\.\\d+)?)', s)\n",
    "    \n",
    "    parts = []\n",
    "    for token in tokens:\n",
    "        if not token: \n",
    "            continue\n",
    "        if re.fullmatch(r'\\d+(?:\\.\\d+)?', token):\n",
    "            # число: целая часть + опциональная дробная (ровно 1 цифра)\n",
    "            parts.append(r'\\d+(?:\\.\\d)?')\n",
    "        elif token.isspace():\n",
    "            # пробелы -> \\s*\n",
    "            parts.append(r'\\s*')\n",
    "        else:\n",
    "            # все остальные символы берём как есть (экранируем)\n",
    "            # пробелы внутри них отдельно не обрабатываем\n",
    "            parts.append(re.escape(token).replace(r'\\ ', r'\\s*'))\n",
    "    \n",
    "    # Собираем всё и добавляем якоря\n",
    "    return '^' + ''.join(parts) + '$'\n",
    "\n",
    "examples = [\"2.3 x 59.9 x 45\", \"24.5x59x60.5\", \"240x320\", \"240x320 см\"]\n",
    "i = 0\n",
    "for ex in examples:\n",
    "    if i>0 and make_regex_from_string(ex, False) == reg_ex:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"False\")\n",
    "    reg_ex = make_regex_from_string(ex, False)\n",
    "    print(ex, \"->\", reg_ex)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482a884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e41b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Написать функцию для подкраски и выявления преобладающего формата\n",
    "def highlight_dominant_format(file_path: str, pair, flag: Literal[\"similar\", \"analogic\"] = \"similar\"):\n",
    "    \"\"\"\n",
    "    Выявляет и подкрашивает преобладающий формат ячеек в Excel-файле.\n",
    "    \n",
    "    :param file_path: путь к файлу Excel\n",
    "    :param flag: тип анализа (\"similar\" или \"analogic\")\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    # Выявить преобладающий формат ячеек в паре. Формат - это регулярное выражение, в котором все цифры заменены на ?\n",
    "    format_regex = r\"\\D*?\\d+.*?\"\n",
    "    col1, col2 = pair\n",
    "    series1 = df[col1]\n",
    "    series2 = df[col2]\n",
    "    # Применить регулярное выражение к каждой ячейке\n",
    "    formatted1 = series1.astype(str).replace(format_regex, \"?\", regex=True)\n",
    "    formatted2 = series2.astype(str).replace(format_regex, \"?\", regex=True)\n",
    "    # Найти преобладающий формат\n",
    "    dominant_format = formatted1.value_counts().idxmax()\n",
    "    # Подсветить ячейки в Excel\n",
    "    highlight_cells(file_path, col1, col2, dominant_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52bf3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\d.\\d\\d см\n"
     ]
    }
   ],
   "source": [
    "def to_regex(s: str) -> str:\n",
    "    # заменяем все цифры на \\d\n",
    "    return re.sub(r\"\\d\", r\"\\\\d\", s)\n",
    "\n",
    "print(to_regex(\"1.23 см\"))  # \\d.\\d\\d см\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
