{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5a892e",
   "metadata": {},
   "source": [
    "Шаблон для запуска парсинга: \\\n",
    "python parsing.py 2 True housedorf urls_housedorf.txt example.xlsx result_housedorf.xlsx \n",
    "python parsing.py 2 False falmec urls_falmec.txt result_housedorf.xlsx result.xlsx\n",
    "\n",
    "python generate_syn_report.py housedorf syn_report_housedorf.xlsx\n",
    "python generate_syn_report.py falmec syn_report_falmec.xlsx\n",
    "\n",
    "python synonyms_dict_update.py syn_report_housedorf.xlsx\n",
    "python synonyms_dict_update.py syn_report_falmec.xlsx\n",
    "\n",
    "python parsing.py 2 True housedorf urls_housedorf.txt example.xlsx result_housedorf.xlsx \n",
    "\n",
    "python parsing.py 2 False falmec urls_falmec.txt result_housedorf.xlsx result.xlsx\n",
    "\n",
    "python compare.py falmec housedorf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf61f80",
   "metadata": {},
   "source": [
    "# Программа parsing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f4adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import PatternFill, Alignment\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import os\n",
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "synonyms_path='synonyms.txt'\n",
    "all_characteristics_path='all_characteristics.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d17c34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------Спарсить данные------------------------------------------------------------------------\n",
    "\n",
    "# Функции для парсинга\n",
    "def parse_korting_page(html_code):\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    tabs_lists = soup.find_all('ul', class_='tabs-settings__list')\n",
    "    data = {}\n",
    "\n",
    "    for ul in tabs_lists:\n",
    "        for li in ul.find_all('li'):\n",
    "            text = li.get_text(strip=True, separator=\"; \")\n",
    "            split_text = text.split(\":;\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                key, value = split_text\n",
    "                data[key.strip()] = value.strip()\n",
    "            else:\n",
    "                data[split_text[0].strip()] = \"\"\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_dedietrich_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='characteristics__row'):\n",
    "        name_span = row.find('span', class_='characteristics__name')\n",
    "        value_span = row.find('span', class_='characteristics__property')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_vzug_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "    names = soup.find_all('td', class_='cell_name')\n",
    "    values = soup.find_all('td', class_='cell_value')\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for name, value in zip(names, values):\n",
    "        name_span = name.find('span')\n",
    "        value_span = value.find('span')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_falmec_page(html_code: str) -> dict:\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Проходим по всем div с классом characteristics__row\n",
    "    for row in soup.find_all('div', class_='characteristics__row'):\n",
    "        name_span = row.find('span', class_='characteristics__name')\n",
    "        value_span = row.find('span', class_='characteristics__property')\n",
    "        \n",
    "        if name_span and value_span:\n",
    "            key = name_span.find(text=True, recursive=False)\n",
    "            value = value_span.find(text=True, recursive=False)\n",
    "\n",
    "            # Если нет простого текста, ищем <ul> и собираем <li>\n",
    "            if (value == '' or not value.strip()) and value_span.find('ul'):\n",
    "                li_items = value_span.find_all('li')\n",
    "                value = '; '.join(li.get_text(strip=True) for li in li_items)\n",
    "\n",
    "            if key and value:\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_first_visible_text(tag):\n",
    "    for desc in tag.descendants:\n",
    "        if isinstance(desc, str):  # Это NavigableString\n",
    "            text = desc.strip()\n",
    "            if text:\n",
    "                return text\n",
    "    return None\n",
    "\n",
    "def clean_value_div(value_div):\n",
    "    # 1. Удалить все <span>\n",
    "    for span in value_div.find_all(\"span\"):\n",
    "        span.decompose()\n",
    "\n",
    "    # 2. Разделить по <br> — создаём список на основе HTML с разделителем\n",
    "    parts = str(value_div).split('<br')\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for part_html in parts:\n",
    "        # Восстанавливаем HTML-тег <br>, если он был отрезан\n",
    "        if not part_html.startswith('>'):\n",
    "            part_html = '<br' + part_html\n",
    "\n",
    "        part_soup = BeautifulSoup(part_html, 'html.parser')\n",
    "\n",
    "        # 3. Найти первый видимый текст\n",
    "        for desc in part_soup.descendants:\n",
    "            if isinstance(desc, NavigableString):\n",
    "                text = desc.strip()\n",
    "                if text:\n",
    "                    values.append(text)\n",
    "                    break  # только первое вхождение\n",
    "\n",
    "    # 4. Склеить с разделителем \"; \"\n",
    "    return \"; \".join(values)\n",
    "\n",
    "def parse_hausedorf_page(html_code):\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    fields = soup.find_all('div', class_='detail-properties__field')\n",
    "    data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        name_div = field.find('div', class_='detail-properties__name')\n",
    "        value_div = field.find('div', class_='detail-properties__value')\n",
    "\n",
    "        if name_div and value_div:\n",
    "            key = extract_first_visible_text(name_div)\n",
    "            value = clean_value_div(value_div)\n",
    "\n",
    "            if key and value:\n",
    "                data[re.sub(r'\\s+', ' ', key).strip()] = value.replace(\">\\n\", \"\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_src(file_path, parser_func):\n",
    "    \"\"\"\n",
    "    Универсальный загрузчик таблицы характеристик с разных сайтов.\n",
    "\n",
    "    :param file_path: путь к файлу с URL (один URL на строку)\n",
    "    :param parser_func: функция, которая получает HTML-код и возвращает словарь {ключ: значение}\n",
    "    :return: DataFrame с объединёнными результатами\n",
    "    \"\"\"\n",
    "    http = urllib3.PoolManager()\n",
    "    df_all = pd.DataFrame()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        urls = [line.strip() for line in f]\n",
    "\n",
    "    n_rows = 0\n",
    "    for url in tqdm(urls):\n",
    "        if url:\n",
    "            try:\n",
    "                response = http.request('GET', url)\n",
    "                html_code = response.data.decode()\n",
    "                data = parser_func(html_code) \n",
    "\n",
    "                if not isinstance(data, dict):\n",
    "                    raise ValueError(\"parser_func должна возвращать словарь!\")\n",
    "\n",
    "                row_df = pd.DataFrame([data])\n",
    "                df_all = pd.concat([df_all, row_df], ignore_index=True)\n",
    "\n",
    "                if len(df_all) == 1:\n",
    "                    empty_rows = pd.DataFrame(np.nan, index=range(n_rows), columns=df_all.columns)\n",
    "                    df_all = pd.concat([empty_rows, df_all], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке {url}: {e}\")\n",
    "        else:\n",
    "            if len(df_all.columns) > 0:                \n",
    "                df_all.loc[len(df_all)] = None\n",
    "            else:\n",
    "                n_rows += 1\n",
    "\n",
    "    return df_all.where(pd.notnull(df_all), None)\n",
    "\n",
    "# ---------------------------------------------Записать и вернуть дополненный названиями номенклатуры DataFrame---------------------------------------------\n",
    "\n",
    "def write_dest(ref_file_path, result_file_path, df_src, start_row_index):\n",
    "    # Путь к файлу Excel\n",
    "    wb = load_workbook(ref_file_path)\n",
    "    ws = wb.active  # или wb['SheetName']\n",
    "\n",
    "    # Поля файла-приёмника\n",
    "    row_header = [cell.value for cell in ws[1]]\n",
    "\n",
    "    # Сопоставление колонок\n",
    "    src_cols_lower = {col.lower(): col for col in df_src.columns}\n",
    "    ws_cols_lower = {i: str(header).strip().lower() if header else \"\" for i, header in enumerate(row_header)}\n",
    "    matched_columns = []\n",
    "    common_cols = set()\n",
    "    nomenclature_col_idx = None\n",
    "\n",
    "    for col_idx, header_lower in ws_cols_lower.items():\n",
    "        if header_lower == \"номенклатура\":\n",
    "            nomenclature_col_idx = col_idx\n",
    "        if header_lower in src_cols_lower:\n",
    "            matched_columns.append((col_idx, src_cols_lower[header_lower]))\n",
    "            common_cols.add(src_cols_lower[header_lower])\n",
    "\n",
    "    if nomenclature_col_idx is None:\n",
    "        raise ValueError(\"Колонка 'Номенклатура' не найдена в файле-приёмнике\")\n",
    "\n",
    "    # Считываем значения \"Номенклатура\"\n",
    "    nomenclature_values = []\n",
    "    for i in range(len(df_src)):\n",
    "        cell_value = ws.cell(row=start_row_index + i, column=nomenclature_col_idx + 1).value\n",
    "        nomenclature_values.append(cell_value)\n",
    "\n",
    "    # Запись данных\n",
    "    for i, (_, row_src) in enumerate(df_src.iterrows()):\n",
    "        for col_idx, src_col in matched_columns:\n",
    "            cell = ws.cell(row=start_row_index + i, column=col_idx + 1)\n",
    "            if cell.value in [None, \"\"]:\n",
    "                cell.value = row_src[src_col]\n",
    "\n",
    "    # Сохранение\n",
    "    wb.save(result_file_path)\n",
    "\n",
    "    # Подготовка выходного DataFrame\n",
    "    result_df = df_src[list(common_cols)].copy()\n",
    "    result_df.insert(0, \"Номенклатура\", pd.Series(nomenclature_values))\n",
    "\n",
    "    return result_df, common_cols\n",
    "\n",
    "# -------------------------------------------Сохранить незаписанные данные в дополнительные колонки или отдельный файл-------------------------------------------\n",
    "\n",
    "def append_dataframe_to_excel(df: pd.DataFrame, file_path: str, result_path: str, start_row: int):\n",
    "    # Проверка, существует ли файл\n",
    "    if os.path.exists(file_path):\n",
    "        wb = load_workbook(file_path)\n",
    "    else:\n",
    "        # Если файла нет, создаем новый\n",
    "        wb = Workbook()\n",
    "    \n",
    "    ws = wb.active\n",
    "\n",
    "    # Найдём первую пустую ячейку в первой строке\n",
    "    col_index = 1\n",
    "    while ws.cell(row=1, column=col_index).value is not None:\n",
    "        col_index += 1\n",
    "\n",
    "    # Записываем названия колонок DataFrame в первую строку, начиная с найденной колонки\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        cell = ws.cell(row=1, column=col_index + i, value=col_name)\n",
    "        if len(col_name.split('_')) > 1:\n",
    "            cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')\n",
    "            \n",
    "\n",
    "    # Автонастройка ширины столбцов по первой строке\n",
    "    for col_idx, cell in enumerate(ws[1], start=col_index):\n",
    "        max_length = len(str(cell.value)) if cell.value else 0\n",
    "        col_letter = cell.column_letter\n",
    "        ws.column_dimensions[col_letter].width = max_length + 2  # +2 для отступа\n",
    "\n",
    "    # Применение стилей и переносов\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, max_col=ws.max_column):\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical='center')  # включаем перенос текста\n",
    "\n",
    "    # Записываем данные DataFrame начиная со start_row\n",
    "    for row_offset, row in enumerate(dataframe_to_rows(df, index=False, header=False)):\n",
    "        for i, value in enumerate(row):\n",
    "            ws.cell(row=start_row + row_offset, column=col_index + i, value=value)\n",
    "\n",
    "    # Сохраняем файл\n",
    "    wb.save(result_path)\n",
    "\n",
    "def save_missing(df1, filepath):\n",
    "\n",
    "    # Создаём ExcelWriter\n",
    "    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "        row = 0\n",
    "\n",
    "        # Запись df1\n",
    "        df1.to_excel(writer, index=False, startrow=row)\n",
    "        row += len(df1) + 2  # +1 за заголовок, +1 за пустую строку\n",
    "\n",
    "        # Автоматическая установка ширины колонок\n",
    "        worksheet = writer.sheets['Sheet1']\n",
    "        for column_cells in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column = column_cells[0].column\n",
    "            for cell in column_cells:\n",
    "                if cell.value:\n",
    "                    max_length = max(max_length, len(str(cell.value)))\n",
    "            adjusted_width = max_length + 2\n",
    "            worksheet.column_dimensions[get_column_letter(column)].width = adjusted_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестировать функцию парсинга\n",
    "def test_parser(url, parser_func):\n",
    "    http = urllib3.PoolManager()\n",
    "    df_all = pd.DataFrame()\n",
    "    response = http.request('GET', url)\n",
    "    html_code = response.data.decode()\n",
    "    data = parser_func(html_code)\n",
    "    return data\n",
    "\n",
    "data = test_parser('https://vzug-shop.ru/catalog/kholodilniki/vstraivaemaya-morozilnaya-kamera-v-zug-freezer-v4000-178n-fr4t-53003/', parse_vzug_page)\n",
    "print(len(data))  # Должно вернуть количество характеристик, например 10\n",
    "data # Выводим результат парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64a089da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def list_synonyms_comparison(list1, list2):\n",
    "    return [are_synonyms(word1, word2) for word1, word2 in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "764e71c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, False]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Протестировать подбор синонимов\n",
    "list1 = ['Машина', 'Счастливый', 'Работать']\n",
    "list2 = ['автомобиля', 'счастливый', 'работник']\n",
    "list_synonyms_comparison(list1, list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "571ce816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------Добавить функцию для обеспечения правильного дописывания данных основываясь на утверждённых синонимах-----------------------------\n",
    "\n",
    "def parse_custom_dict_line(line):\n",
    "    \"\"\"\n",
    "    Разбирает строку из словаря: <характеристика>: <синоним1>; <синоним2>; ...| <антисиноним1>, <антисиноним2>, ...\n",
    "    \"\"\"\n",
    "    base, *rest = line.strip().split(':')\n",
    "    if not rest:\n",
    "        return base.strip(), set(), set()\n",
    "    syn_ant = rest[0].split('|')\n",
    "    synonyms = set(map(str.strip, syn_ant[0].split(';'))) if syn_ant[0] else set()\n",
    "    antisynonyms = set(map(str.strip, syn_ant[1].split(','))) if len(syn_ant) > 1 else set()\n",
    "    return base.strip(), synonyms, antisynonyms\n",
    "\n",
    "def load_existing_synonyms(file_path):\n",
    "    syn_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            base, synonyms, antisynonyms = parse_custom_dict_line(line)\n",
    "            syn_dict[base] = {'synonyms': synonyms, 'antisynonyms': antisynonyms}\n",
    "    return syn_dict\n",
    "\n",
    "def rename_columns_with_syn_dict(df, syn_dict_path, all_1c_chars_path):\n",
    "    # Загрузка словаря\n",
    "    synonyms_dict = load_existing_synonyms(syn_dict_path)\n",
    "    all_chars = pd.read_excel(all_1c_chars_path, header=None).iloc[0].astype(str).tolist()\n",
    "    all_chars_lower = [char.lower() for char in all_chars]  # Приводим к нижнему регистру для сравнения\n",
    "\n",
    "    # Удаление дублирующихся колонок по имени\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # Поиск синонимичных имен колонок через словарь\n",
    "    df_expanded = df.copy()\n",
    "    unsyn_set = set()\n",
    "    for col in df.columns:\n",
    "        col_splitted = col.split(':')[0]\n",
    "        # Если колонка уже есть в 1С - пропускаем её\n",
    "        if col_splitted.lower() in all_chars_lower:\n",
    "            df_expanded[col_splitted.upper()] = df[col]\n",
    "            df_expanded.drop(columns=[col], inplace=True)\n",
    "            continue\n",
    "\n",
    "        is_syn = False\n",
    "        for synonym_key, syn_data in synonyms_dict.items():\n",
    "            syn_set = syn_data.get(\"synonyms\", set())\n",
    "            if col_splitted in syn_set:\n",
    "                # Добавляем колонку с именем synonym_key, если она ещё не существует\n",
    "                if synonym_key not in df_expanded.columns:\n",
    "                    df_expanded[synonym_key] = df[col]\n",
    "                    is_syn = True\n",
    "        if is_syn:\n",
    "            # Если колонка была переименована, удаляем оригинальную и не ищем полусинонимы\n",
    "            df_expanded.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            is_half_syn = False\n",
    "            for synonym_key, syn_data in synonyms_dict.items():\n",
    "                syn_set = syn_data.get(\"synonyms\", set())\n",
    "                if col_splitted in set(map(lambda x: x.split(\"*\")[1] if len(x.split(\"*\")) > 1 else x, syn_set)):\n",
    "                    # Добавляем колонку с именем synonym_key, если она ещё не существует\n",
    "                    if synonym_key not in df_expanded.columns:\n",
    "                        df_expanded[f\"{synonym_key}_{col_splitted}\"] = df[col]\n",
    "                        is_half_syn = True\n",
    "\n",
    "            if is_half_syn:\n",
    "                df_expanded.drop(columns=[col], inplace=True)\n",
    "            else:\n",
    "                is_antisyn = False\n",
    "                for synonym_key, syn_data in synonyms_dict.items():\n",
    "                    antisyn_set = syn_data.get(\"antisynonyms\", set())\n",
    "                    if col_splitted in antisyn_set:\n",
    "                        is_antisyn = True\n",
    "                        break\n",
    "\n",
    "                if not is_antisyn:\n",
    "                    unsyn_set.add(col_splitted)\n",
    "\n",
    "    return df_expanded, unsyn_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6bcdd",
   "metadata": {},
   "source": [
    "# Варианты запуска парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194aa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------Запуск парсинга и сохранение результатов-----------------------------------------------------------\n",
    "\n",
    "# Протестировать запись одной строки с двух сайтов\n",
    "from argparse import Namespace\n",
    "\n",
    "# housedorf\n",
    "args = Namespace(start_row=33, append=True, site='housedorf', urls_source='input_example/url_housedorf.txt', input_path='input_example/example.xlsx', output_path='result_housedorf.xlsx')\n",
    "if not args.output_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "# Проверка, не повреждены ли входные файлы\n",
    "wb = load_workbook(args.input_path)\n",
    "\n",
    "if args.site == 'korting':\n",
    "    df_src = create_src(args.urls_source, parse_korting_page)\n",
    "elif args.site == 'housedorf':\n",
    "    df_src = create_src(args.urls_source, parse_hausedorf_page)\n",
    "elif args.site == 'dedietrich':\n",
    "    df_src = create_src(args.urls_source, parse_dedietrich_page)\n",
    "elif args.site == 'falmec':\n",
    "    df_src = create_src(args.urls_source, parse_falmec_page)\n",
    "elif args.site == 'vzug':\n",
    "    df_src = create_src(args.urls_source, parse_vzug_page)\n",
    "else:\n",
    "    raise ValueError(\"There're no parse function for this site\")\n",
    "\n",
    "df_src, unsyn_set = rename_columns_with_syn_dict(df_src, synonyms_path, all_characteristics_path) # Для обеспечения правильного дописывания данных и для корректного входа к функции, \n",
    "                                                                        # генерирующей отчёт\n",
    "resultdf, com_cols = write_dest(args.input_path, args.output_path, df_src, args.start_row)\n",
    "resultdf.to_parquet(f\"{args.site}_auxiliary.parquet\")\n",
    "\n",
    "com_cols = resultdf.columns.intersection(df_src.columns)\n",
    "missingdf = df_src.drop(columns=com_cols).copy()\n",
    "print(unsyn_set)\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'w') as f:\n",
    "    f.write('; '.join(map(str, unsyn_set)))\n",
    "\n",
    "if args.append:\n",
    "    append_dataframe_to_excel(missingdf, args.output_path, args.output_path, args.start_row)\n",
    "else:\n",
    "    missingdf.insert(0, 'Номенклатура', resultdf['Номенклатура'].copy())\n",
    "    save_missing(missingdf, f'missing_{args.site}.xlsx')\n",
    "\n",
    "# korting\n",
    "args = Namespace(start_row=34, append=False, site='korting', urls_source='input_example/url_korting.txt', input_path='result_housedorf.xlsx', output_path='result.xlsx')\n",
    "if not args.output_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "# Проверка, не повреждены ли входные файлы\n",
    "wb = load_workbook(args.input_path)\n",
    "\n",
    "if args.site == 'korting':\n",
    "    df_src = create_src(args.urls_source, parse_korting_page)\n",
    "elif args.site == 'housedorf':\n",
    "    df_src = create_src(args.urls_source, parse_hausedorf_page)\n",
    "elif args.site == 'dedietrich':\n",
    "    df_src = create_src(args.urls_source, parse_dedietrich_page)\n",
    "elif args.site == 'falmec':\n",
    "    df_src = create_src(args.urls_source, parse_falmec_page)\n",
    "elif args.site == 'vzug':\n",
    "    df_src = create_src(args.urls_source, parse_vzug_page)\n",
    "else:\n",
    "    raise ValueError(\"There're no parse function for this site\")\n",
    "\n",
    "df_src, unsyn_set = rename_columns_with_syn_dict(df_src, synonyms_path, all_characteristics_path) # Для обеспечения правильного дописывания данных и для корректного входа к функции, \n",
    "                                                                        # генерирующей отчёт\n",
    "resultdf, com_cols = write_dest(args.input_path, args.output_path, df_src, args.start_row)\n",
    "resultdf.to_parquet(f\"{args.site}_auxiliary.parquet\")\n",
    "\n",
    "com_cols = resultdf.columns.intersection(df_src.columns)\n",
    "missingdf = df_src.drop(columns=com_cols).copy()\n",
    "print(unsyn_set)\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'w') as f:\n",
    "    f.write('; '.join(map(str, unsyn_set)))\n",
    "\n",
    "if args.append:\n",
    "    append_dataframe_to_excel(missingdf, args.output_path, args.output_path, args.start_row)\n",
    "else:\n",
    "    missingdf.insert(0, 'Номенклатура', resultdf['Номенклатура'].copy())\n",
    "    save_missing(missingdf, f'missing_{args.site}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ef97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Актуальная версия запуска\n",
    "from argparse import Namespace\n",
    "\n",
    "# housedorf\n",
    "args = Namespace(start_row=7, append=False, site='falmec', urls_source='url_falmec.txt', input_path='result_housedorf.xlsx', output_path='result.xlsx')\n",
    "if not args.output_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "# Проверка, не повреждены ли входные файлы\n",
    "wb = load_workbook(args.input_path)\n",
    "\n",
    "if args.site == 'korting':\n",
    "    df_src = create_src(args.urls_source, parse_korting_page)\n",
    "elif args.site == 'housedorf':\n",
    "    df_src = create_src(args.urls_source, parse_hausedorf_page)\n",
    "elif args.site == 'dedietrich':\n",
    "    df_src = create_src(args.urls_source, parse_dedietrich_page)\n",
    "elif args.site == 'falmec':\n",
    "    df_src = create_src(args.urls_source, parse_falmec_page)\n",
    "elif args.site == 'vzug':\n",
    "    df_src = create_src(args.urls_source, parse_vzug_page)\n",
    "else:\n",
    "    raise ValueError(\"There're no parse function for this site\")\n",
    "\n",
    "df_src, unsyn_set = rename_columns_with_syn_dict(df_src, synonyms_path, all_characteristics_path) # Для обеспечения правильного дописывания данных и для корректного входа к функции, \n",
    "                                                                        # генерирующей отчёт\n",
    "resultdf, com_cols = write_dest(args.input_path, args.output_path, df_src, args.start_row)\n",
    "resultdf.to_parquet(f\"{args.site}_auxiliary.parquet\")\n",
    "\n",
    "com_cols = resultdf.columns.intersection(df_src.columns)\n",
    "missingdf = df_src.drop(columns=com_cols).copy()\n",
    "print(unsyn_set)\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('; '.join(map(str, unsyn_set)))\n",
    "\n",
    "if args.append:\n",
    "    append_dataframe_to_excel(missingdf, args.output_path, args.output_path, args.start_row)\n",
    "else:\n",
    "    # Условие: имена столбцов, у которых есть хотя бы один символ \"_\"\n",
    "    columns_with_underscore = [col for col in missingdf.columns if len(col.split(\"_\")) > 1]\n",
    "    columns_without_underscore = [col for col in missingdf.columns if len(col.split(\"_\")) <= 1]\n",
    "\n",
    "    # Делим missingdf на два\n",
    "    df_with_underscore = missingdf[columns_with_underscore]\n",
    "    df_without_underscore = missingdf[columns_without_underscore]\n",
    "    df_without_underscore.insert(0, 'Номенклатура', resultdf['Номенклатура'].copy())\n",
    "    save_missing(df_without_underscore, f'missing_{args.site}.xlsx')\n",
    "    append_dataframe_to_excel(df_with_underscore, args.output_path, args.output_path, args.start_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc06f5",
   "metadata": {},
   "source": [
    "# Программа generate_syn_report.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2cfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "synonyms_path='synonyms.txt'\n",
    "all_characteristics_path='all_characteristics.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a23905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def list_synonyms_comparison(list1, list2):\n",
    "    return [are_synonyms(word1, word2) for word1, word2 in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cdbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------Добавить функцию для создания отчёта о сопоставлении колонок тем, что уже существуют в 1С-----------------------------------\n",
    "\n",
    "def parse_custom_dict_line(line):\n",
    "    \"\"\"\n",
    "    Разбирает строку из словаря: <характеристика>: <синоним1>; <синоним2>; ...| <антисиноним1>, <антисиноним2>, ...\n",
    "    \"\"\"\n",
    "    base, *rest = line.strip().split(':')\n",
    "    if not rest:\n",
    "        return base.strip(), set(), set()\n",
    "    syn_ant = rest[0].split('|')\n",
    "    synonyms = set(map(str.strip, syn_ant[0].split(';'))) if syn_ant[0] else set()\n",
    "    antonyms = set(map(str.strip, syn_ant[1].split(','))) if len(syn_ant) > 1 else set()\n",
    "    return base.strip(), synonyms, antonyms\n",
    "\n",
    "def load_existing_synonyms(file_path):\n",
    "    syn_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            base, synonyms, antonyms = parse_custom_dict_line(line)\n",
    "            syn_dict[base] = {'synonyms': synonyms, 'antonyms': antonyms}\n",
    "    return syn_dict\n",
    "\n",
    "def tokenize(text):\n",
    "    return set(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "def are_words_possibly_synonyms(words1, words2, are_synonims_func):\n",
    "    pairs = product(words1, words2)\n",
    "    for w1, w2 in pairs:\n",
    "        if w1 == w2:\n",
    "            return True\n",
    "        if are_synonims_func(w1, w2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_synonym_report(existing_dict_path, all_1c_chars_path, new_chars, are_synonims_func, output_excel_path):\n",
    "    custom_dict = load_existing_synonyms(existing_dict_path)\n",
    "    all_chars = pd.read_excel(all_1c_chars_path, header=None).iloc[0].astype(str).tolist()\n",
    "    result_rows = []\n",
    "\n",
    "    for c1 in tqdm(all_chars):\n",
    "        for c2 in new_chars:\n",
    "            if c1 == c2:\n",
    "                continue\n",
    "            tokens1 = tokenize(c1)\n",
    "            tokens2 = tokenize(c2)\n",
    "            if are_words_possibly_synonyms(tokens1, tokens2, are_synonims_func):\n",
    "                result_rows.append((c1, c2, None))  # None для ручной отметки\n",
    "\n",
    "    df_result = pd.DataFrame(result_rows, columns=[\"base_char\", \"compared_char\", \"label\"])\n",
    "    df_result.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d1c109",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xce in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthere\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not enough arguments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munaccepted_syn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39msite\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 10\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m unsyn_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, data\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     14\u001b[0m generate_synonym_report(\n\u001b[0;32m     15\u001b[0m     synonyms_path,\n\u001b[0;32m     16\u001b[0m     all_characteristics_path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     args\u001b[38;5;241m.\u001b[39msynonyms_report_path\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xce in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------Запуск генерации-----------------------------------------------------------------------\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(site='housedorf', synonyms_report_path='syn_report_housedorf.xlsx')\n",
    "if not args.synonyms_report_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "with open(f'unaccepted_syn_{args.site}.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "unsyn_list = list(map(str, data.strip().split('; ')))\n",
    "\n",
    "generate_synonym_report(\n",
    "    synonyms_path,\n",
    "    all_characteristics_path,\n",
    "    unsyn_list,\n",
    "    are_synonyms,\n",
    "    args.synonyms_report_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c5bd6",
   "metadata": {},
   "source": [
    "# Программа synonyms_dict_update.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6afd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import argparse\n",
    "synonyms_path='synonyms.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd4a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------Обновить словарь синонимов из Excel--------------------------------------------------------------\n",
    "def load_synonym_dict(dict_path):\n",
    "    syn_dict = defaultdict(lambda: {\"synonyms\": set(), \"antisynonyms\": set()})\n",
    "    if not os.path.exists(dict_path):\n",
    "        return syn_dict\n",
    "    \n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            key, rest = line.strip().split(\":\", 1)\n",
    "            syn_part, *anti_part = rest.strip().split(\"|\")\n",
    "            syns = set(map(str.strip, syn_part.strip().split(\";\"))) if syn_part.strip() else set()\n",
    "            antis = set(map(str.strip, anti_part[0].strip().split(\";\"))) if anti_part and anti_part[0].strip() else set()\n",
    "            syn_dict[key.strip()][\"synonyms\"].update(syns)\n",
    "            syn_dict[key.strip()][\"antisynonyms\"].update(antis)\n",
    "    return syn_dict\n",
    "\n",
    "\n",
    "def save_synonym_dict(syn_dict, dict_path):\n",
    "    with open(dict_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for key in sorted(syn_dict.keys()):\n",
    "            syns = \"; \".join(sorted(syn_dict[key][\"synonyms\"]))\n",
    "            antis = \"; \".join(sorted(syn_dict[key][\"antisynonyms\"]))\n",
    "            line = f\"{key}: {syns} | {antis}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "\n",
    "def update_synonym_dict_from_excel(excel_path, dict_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    if not {\"base_char\", \"compared_char\", \"label\"}.issubset(df.columns):\n",
    "        raise ValueError(\"Excel должен содержать столбцы: base_char, compared_char, label\")\n",
    "    \n",
    "    syn_dict = load_synonym_dict(dict_path)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        base = row[\"base_char\"].strip().split(\":\")[0]\n",
    "        comp = row[\"compared_char\"].strip().split(\":\")[0]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        if label == 1:\n",
    "            syn_dict[base][\"synonyms\"].add(comp)\n",
    "        elif label == 0.5:\n",
    "            syn_dict[base][\"synonyms\"].add(\"*\"+comp)\n",
    "        elif label == 0 or pd.isna(label):\n",
    "            syn_dict[base][\"antisynonyms\"].add(comp)\n",
    "        else:\n",
    "            continue  # Пропустить некорректные значения\n",
    "\n",
    "    # Удалить пересекающиеся значения\n",
    "    for base in syn_dict:\n",
    "        overlap1 = syn_dict[base][\"synonyms\"] & syn_dict[base][\"antisynonyms\"]\n",
    "        overlap2 = set(map(lambda x: x.split(\"*\")[1] if len(x.split(\"*\")) > 1 else x, syn_dict[base][\"synonyms\"])) & syn_dict[base][\"antisynonyms\"]\n",
    "        syn_dict[base][\"antisynonyms\"] -= overlap1\n",
    "        syn_dict[base][\"antisynonyms\"] -= overlap2\n",
    "\n",
    "    save_synonym_dict(syn_dict, dict_path)\n",
    "    print(f\"Обновлённый словарь сохранён в {dict_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d8486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обновлённый словарь сохранён в synonyms.txt\n"
     ]
    }
   ],
   "source": [
    "# Запустить обновление файла-словаря\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "# housedorf\n",
    "args = Namespace(report_path='syn_report_housedorf.xlsx')\n",
    "if not args.report_path:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "\n",
    "update_synonym_dict_from_excel(args.report_path, synonyms_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестировать обновление файла-словаря\n",
    "syn_dict = load_synonym_dict('synonyms.txt')\n",
    "for base in syn_dict:\n",
    "    if len(syn_dict[base]['synonyms']) > 0:\n",
    "        print(f\"{base}: {syn_dict[base]['synonyms']}, {syn_dict[base]['antisynonyms']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee85889",
   "metadata": {},
   "source": [
    "# Программа compare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import PatternFill, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import argparse\n",
    "from itertools import product\n",
    "from ruwordnet import RuWordNet\n",
    "import pymorphy2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f42b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------Добавить функции для сопоставления синонимов---------------------------------------------------------\n",
    "\n",
    "wordnet = RuWordNet()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_normal_form(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def are_synonyms(word1, word2):\n",
    "    lemma1 = get_normal_form(word1)\n",
    "    lemma2 = get_normal_form(word2)\n",
    "\n",
    "    synsets1 = wordnet.get_synsets(lemma1)\n",
    "    synsets2 = wordnet.get_synsets(lemma2)\n",
    "\n",
    "    # Сравниваем наличие общих лемм в синсетах\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.id == s2.id:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def tokenize(text):\n",
    "    return set(re.findall(r'\\w+', text.lower()))\n",
    "\n",
    "def are_words_possibly_synonyms(words1, words2, are_synonims_func):\n",
    "    pairs = product(words1, words2)\n",
    "    for w1, w2 in pairs:\n",
    "        if w1 == w2:\n",
    "            return True\n",
    "        if are_synonims_func(w1, w2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b87f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------Сравнить записанные данные-------------------------------------------------------------------\n",
    "\n",
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, name1: str = 'df1', name2: str = 'df2') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Сравнивает два DataFrame по общим столбцам, включая \"Номенклатура\" как ключ.\n",
    "    Добавляет префиксы к столбцам (кроме \"Номенклатура\") и формирует колонку diff_columns.\n",
    "    \"\"\"\n",
    "    if 'Номенклатура' not in df1.columns or 'Номенклатура' not in df2.columns:\n",
    "        raise ValueError(\"Оба DataFrame должны содержать колонку 'Номенклатура'\")\n",
    "\n",
    "    # Определим общие характеристики (кроме \"Номенклатура\")\n",
    "    common_columns = df1.columns.intersection(df2.columns).difference(['Номенклатура'])\n",
    "\n",
    "    # Сузим входные DataFrame'ы до нужных колонок\n",
    "    df1_reduced = df1[['Номенклатура'] + list(common_columns)].copy()\n",
    "    df2_reduced = df2[['Номенклатура'] + list(common_columns)].copy()\n",
    "\n",
    "    # Переименуем характеристики с префиксами, \"Номенклатура\" оставим без изменений\n",
    "    df1_renamed = df1_reduced.rename(columns={col: f'{name1}_{col}' for col in common_columns})\n",
    "    df2_renamed = df2_reduced.rename(columns={col: f'{name2}_{col}' for col in common_columns})\n",
    "\n",
    "    # Объединение по \"Номенклатура\"\n",
    "    df_merged = pd.merge(df1_renamed, df2_renamed, on='Номенклатура', how='outer')\n",
    "\n",
    "    # Функция для сравнения значений по строке\n",
    "    def get_differences(row):\n",
    "        diffs = []\n",
    "        for col in common_columns:\n",
    "            val1 = row.get(f'{name1}_{col}', None)\n",
    "            val2 = row.get(f'{name2}_{col}', None)\n",
    "            if pd.isna(val1) or pd.isna(val2):\n",
    "                continue\n",
    "            else:\n",
    "                tokens1 = tokenize(str(val1))\n",
    "                tokens2 = tokenize(str(val2))\n",
    "                if not are_words_possibly_synonyms(tokens1, tokens2, are_synonyms):\n",
    "                    diffs.append(col)\n",
    "        return ', '.join(diffs)\n",
    "\n",
    "    # Добавление столбца с различиями\n",
    "    df_merged['diff_columns'] = df_merged.apply(get_differences, axis=1)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "# --------------------------------------------------------------Сохранить результат сравнения в excel--------------------------------------------------------------\n",
    "\n",
    "def save_comparison_to_excel(df: pd.DataFrame, filename: str):\n",
    "    red_fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "    yellow_fill = PatternFill(start_color=\"FFEB9C\", end_color=\"FFEB9C\", fill_type=\"solid\")\n",
    "\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    # Записываем DataFrame в Excel\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws.append(r)\n",
    "\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "    diff_col_index = len(headers)\n",
    "    prefix_columns = [col for col in headers if col != 'Номенклатура' and col != 'diff_columns']\n",
    "\n",
    "    # Автонастройка ширины столбцов по первой строке\n",
    "    for col_idx, cell in enumerate(ws[1], start=1):\n",
    "        max_length = len(str(cell.value)) if cell.value else 0\n",
    "        col_letter = cell.column_letter\n",
    "        ws.column_dimensions[col_letter].width = max_length + 2  # +2 для отступа\n",
    "\n",
    "    # Применение стилей и переносов\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, max_col=ws.max_column):\n",
    "        diff_text = row[diff_col_index - 1].value\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical='center')  # включаем перенос текста\n",
    "\n",
    "        if isinstance(diff_text, str) and diff_text.strip():\n",
    "            different_fields = [field.strip() for field in diff_text.split(',')]\n",
    "            for diff_field in different_fields:\n",
    "                for col_idx, col_name in enumerate(headers):\n",
    "                    if col_name.endswith(f\"_{diff_field}\"):\n",
    "                        row[col_idx].fill = yellow_fill\n",
    "        row[diff_col_index - 1].fill = red_fill\n",
    "\n",
    "    wb.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3398e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестировать сравнение двух DataFrame и сохранение результата в excel\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(site1='korting', site2='housedorf')\n",
    "if not args.site2:\n",
    "    raise ValueError(\"there's not enough arguments\")\n",
    "if args.site1 == args.site2:\n",
    "    raise ValueError(\"the arguments coincide, comparison is impossible\")\n",
    "\n",
    "resultdf_1 = pd.read_parquet(f\"{args.site1}_auxiliary.parquet\")\n",
    "resultdf_2 = pd.read_parquet(f\"{args.site2}_auxiliary.parquet\")\n",
    "comp_result = compare_dataframes(resultdf_1, resultdf_2, args.site1, args.site2)\n",
    "\n",
    "save_comparison_to_excel(comp_result, f'comparison_{args.site1}_vs_{args.site2}.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
