{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199239bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 24 примеров\n",
      "\n",
      "Пример данных:\n",
      "Вход: Официальный дистрибьютор\n",
      "0\n",
      "\n",
      "Я ищу...\n",
      "Главная \n",
      "Каталог \n",
      "Вытяжки \n",
      "Встраиваемые вытяжки \n",
      "Встраиваемая в...\n",
      "Цель: Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "Механическое управление с курсором\n",
      "класс энергопотребления...\n",
      "\n",
      "Загрузка модели cointegrated/rut5-small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 65,332,608 || trainable%: 1.0533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 19/19 [00:00<00:00, 101.52 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 92.21 examples/s]\n",
      "C:\\Users\\loban\\AppData\\Local\\Temp\\ipykernel_12980\\2105122154.py:224: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Размеры датасетов:\n",
      "Тренировочных: 19\n",
      "Валидационных: 5\n",
      "\n",
      "============================================================\n",
      "НАЧИНАЕМ ОБУЧЕНИЕ\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 09:53, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.990600</td>\n",
       "      <td>5.466343</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.877100</td>\n",
       "      <td>5.420467</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.734800</td>\n",
       "      <td>5.384649</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.790400</td>\n",
       "      <td>5.365154</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "c:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "c:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "c:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "c:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Модель сохранена в fast_fine_tuned_t5\n",
      "\n",
      "============================================================\n",
      "ТЕСТИРОВАНИЕ\n",
      "============================================================\n",
      "\n",
      "Вход: \n",
      "                \n",
      "Официальный дистрибьютор\n",
      "0\n",
      "\n",
      "Я ищу...\n",
      "Главная \n",
      "Каталог \n",
      "Вытяжки \n",
      "Встраиваемые вытяжки \n",
      "Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "Духовые шкафы\n",
      "Духовые шкафы\n",
      "Компактные духовые шкафы\n",
      "Духовые шкафы с паром\n",
      "Компактные духовые шкафы с паром\n",
      "Паровые шкафы\n",
      "Варочные панели\n",
      "Индукционные варочные панели\n",
      "Варочные панели Домино\n",
      "Электрические варочные панели\n",
      "Газовые варочные панели\n",
      "Комбинированные варочные панели\n",
      "Вытяжки\n",
      "Настенные вытяжки\n",
      "Встраиваемые вытяжки\n",
      "Островные вытяжки\n",
      "Встраиваемые в столешницу вытяжки\n",
      "Холодильные и морозильные шкафы\n",
      "Встраиваемый холодильный шкаф\n",
      "Винные шкафы\n",
      "Встраиваемый морозильный шкаф\n",
      "Встраиваемый холодильно-морозильный шкаф\n",
      "Отдельностоящие холодильные шкафы\n",
      "Посудомоечные машины\n",
      "Микроволновые печи\n",
      "Кофемашины\n",
      "Вакууматоры и подогреватели\n",
      "Стирально-сушильная техника\n",
      "Аксессуары\n",
      "Outlet и НКТ\n",
      "Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "ALT\n",
      "ALT\n",
      "ALT\n",
      "Варианты дизайна\n",
      "Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "Сравнение\n",
      "62 990 ₽\n",
      "Товар под заказ\n",
      "Доставка: БЕСПЛАТНО ДО КВАРТИРЫ\n",
      "заказ\n",
      "\n",
      "\n",
      "\n",
      "Описание\n",
      "Характеристики\n",
      "Документы\n",
      "Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "\n",
      "Механическое управление с курсором\n",
      "Маркировка энергопотребления: класс B\n",
      "Функция \"Вкл./Выкл.\"\n",
      "Функция \"Boost\"\n",
      "Максимальная производительность: 425 м3/ч\n",
      "Уровень шума: мин: 57 дБ(A), макс.: 69 дБ(A)\n",
      "3 уровня мощности, включая Boost\n",
      "Независимое освещение\n",
      "2 x 2,5 Вт светодиодное освещение\n",
      "\n",
      "\n",
      "Посмотреть все вытяжки коллекции PLATINUM\n",
      "\n",
      "Посмотреть все товары коллекции PLATINUM\n",
      "\n",
      "\n",
      "\n",
      "Режим BOOST\n",
      "Режим BOOST\n",
      "Выбирая этот режим вы мгновенно устанавливаете максимальную производительность, чтобы самым эффективным образом удалять большие количества испарений.\n",
      "Светодиодное освещение\n",
      "Светодиодное освещение\n",
      "Этот метод освещения выделяется тем, что потребляет почти на 85% меньше электроэнергии и служит в 20 раз дольше, чем лампа накаливания. Система эффективно освещает все кастрюли на вашей плите и обеспечивает вам мягкое освещение, идеально подходящее для ужина.\n",
      "\n",
      "\n",
      "\n",
      "zakaz@dedietrich-shop.ru\n",
      "\n",
      "Мы в соцсетях:\n",
      "TELEGRAM\tВКонтакте\n",
      "©2026. Магазин французской бытовой техники De Dietrich. Все права защищены\n",
      "\n",
      "            \n",
      "Результат:\n",
      "Описание продукта на основе текста карточки товара, разделив его особенности переносами строки, игнорируй артефакты вроде меню сайта официальный дистрибьютор 0 Я ищу описание продукта на основе текста карточки товара, разделив его особенности переносами строки, игнорируй артефакты вроде меню сайта\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import evaluate\n",
    "\n",
    "# =============== КОНФИГУРАЦИЯ ===============\n",
    "class Config:\n",
    "    input_dir = \"../rag_data/Pages/inputs\"\n",
    "    output_dir = \"../rag_data/Pages/targets\"\n",
    "    model_save_dir = \"fast_fine_tuned_t5\"\n",
    "    model_name = \"cointegrated/rut5-small\"\n",
    "    batch_size = 2\n",
    "    gradient_accumulation_steps = 4\n",
    "    num_epochs = 15\n",
    "    learning_rate = 5e-5\n",
    "    max_input_length = 256\n",
    "    max_target_length = 512\n",
    "    prefix = \"Cгенерируй описание продукта на основе текста карточки товара, разделив его особенности переносами строки, игнорируй артефакты вроде меню сайта: \"\n",
    "    \n",
    "    # LoRA параметры\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    \n",
    "    Path(model_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =============== ЗАГРУЗКА ДАННЫХ ===============\n",
    "def load_data():\n",
    "    pairs = []\n",
    "    input_files = list(Path(Config.input_dir).glob(\"*.txt\"))\n",
    "    output_files = list(Path(Config.output_dir).glob(\"*.txt\"))\n",
    "    \n",
    "    input_dict = {f.stem: f for f in input_files}\n",
    "    output_dict = {f.stem: f for f in output_files}\n",
    "    common_stems = set(input_dict.keys()) & set(output_dict.keys())\n",
    "    \n",
    "    for stem in list(common_stems)[:25]:\n",
    "        try:\n",
    "            with open(input_dict[stem], 'r', encoding='utf-8') as f:\n",
    "                input_text = f.read().strip()\n",
    "            with open(output_dict[stem], 'r', encoding='utf-8') as f:\n",
    "                output_text = f.read().strip()\n",
    "            \n",
    "            if input_text and output_text:\n",
    "                pairs.append({'input': input_text, 'target': output_text})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Загружено {len(pairs)} примеров\")\n",
    "    \n",
    "    if pairs:\n",
    "        print(\"\\nПример данных:\")\n",
    "        print(f\"Вход: {pairs[0]['input'][:100]}...\")\n",
    "        print(f\"Цель: {pairs[0]['target'][:100]}...\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# =============== ПОДГОТОВКА МОДЕЛИ ===============\n",
    "def prepare_model():\n",
    "    print(f\"\\nЗагрузка модели {Config.model_name}...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(Config.model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(Config.model_name)\n",
    "    \n",
    "    # Настройка LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=Config.lora_r,\n",
    "        lora_alpha=Config.lora_alpha,\n",
    "        lora_dropout=Config.lora_dropout,\n",
    "        target_modules=[\"q\", \"v\"],\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# =============== ПОДГОТОВКА ДАТАСЕТА ===============\n",
    "def prepare_datasets(tokenizer, pairs):\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [Config.prefix + text for text in examples['input']]\n",
    "        targets = examples['target']\n",
    "        \n",
    "        # Токенизация входов\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            max_length=Config.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        # Токенизация целей\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                targets,\n",
    "                max_length=Config.max_target_length,\n",
    "                truncation=True,\n",
    "                padding=False\n",
    "            )\n",
    "        \n",
    "        # Заменяем pad_token_id на -100 для игнорирования в loss\n",
    "        labels_input_ids = labels[\"input_ids\"]\n",
    "        for i in range(len(labels_input_ids)):\n",
    "            labels_input_ids[i] = [\n",
    "                (token_id if token_id != tokenizer.pad_token_id else -100) \n",
    "                for token_id in labels_input_ids[i]\n",
    "            ]\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels_input_ids\n",
    "        return model_inputs\n",
    "    \n",
    "    df = pd.DataFrame(pairs)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    tokenized_datasets = split_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=split_dataset[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nРазмеры датасетов:\")\n",
    "    print(f\"Тренировочных: {len(tokenized_datasets['train'])}\")\n",
    "    print(f\"Валидационных: {len(tokenized_datasets['test'])}\")\n",
    "    \n",
    "    return tokenized_datasets\n",
    "\n",
    "# =============== ОСНОВНОЕ ОБУЧЕНИЕ ===============\n",
    "def train():\n",
    "    # Загружаем данные\n",
    "    pairs = load_data()\n",
    "    if len(pairs) == 0:\n",
    "        print(\"Нет данных для обучения!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Подготавливаем модель и токенизатор\n",
    "    model, tokenizer = prepare_model()\n",
    "    \n",
    "    # Подготавливаем датасет\n",
    "    tokenized_datasets = prepare_datasets(tokenizer, pairs)\n",
    "    \n",
    "    # Определяем функцию compute_metrics ВНУТРИ функции train,\n",
    "    # чтобы она имела доступ к переменной tokenizer\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Декодируем предсказания\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        \n",
    "        # Заменяем -100 на pad_token_id для декодирования\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Вычисляем ROUGE метрики\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        \n",
    "        result = rouge.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True,\n",
    "            use_aggregator=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "            \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "            \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "        }\n",
    "    \n",
    "    # Аргументы обучения\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=Config.model_save_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=Config.batch_size,\n",
    "        per_device_eval_batch_size=Config.batch_size,\n",
    "        gradient_accumulation_steps=Config.gradient_accumulation_steps,\n",
    "        num_train_epochs=Config.num_epochs,\n",
    "        learning_rate=Config.learning_rate,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        \n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=Config.max_target_length,\n",
    "        generation_num_beams=2,\n",
    "        \n",
    "        logging_dir=f\"{Config.model_save_dir}/logs\",\n",
    "        logging_steps=5,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        \n",
    "        fp16=False,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "    \n",
    "    # Коллатор данных\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    # Тренер\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"] if len(tokenized_datasets[\"test\"]) > 0 else None,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Запуск обучения\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"НАЧИНАЕМ ОБУЧЕНИЕ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Сохраняем модель\n",
    "        model.save_pretrained(Config.model_save_dir)\n",
    "        tokenizer.save_pretrained(Config.model_save_dir)\n",
    "        print(f\"\\n✓ Модель сохранена в {Config.model_save_dir}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Ошибка при обучении: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# =============== ТЕСТИРОВАНИЕ ===============\n",
    "def test_generation(model, tokenizer, text=None):\n",
    "    \"\"\"Тестирование генерации\"\"\"\n",
    "    if text is None:\n",
    "        text = \"Стиральная машина\"\n",
    "    \n",
    "    model.eval()\n",
    "    input_text = Config.prefix + text\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=Config.max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=Config.max_target_length,\n",
    "            num_beams=3,\n",
    "            temperature=0.9,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# =============== ЗАПУСК В JUPYTER ===============\n",
    "if __name__ == \"__main__\":\n",
    "    # Запускаем обучение\n",
    "    model, tokenizer = train()\n",
    "    \n",
    "    # Тестируем если обучение успешно\n",
    "    if model and tokenizer:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ТЕСТИРОВАНИЕ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        test_texts = [\n",
    "            \"\"\"\n",
    "                \n",
    "Официальный дистрибьютор\n",
    "0\n",
    "\n",
    "Я ищу...\n",
    "Главная \n",
    "Каталог \n",
    "Вытяжки \n",
    "Встраиваемые вытяжки \n",
    "Встраиваемая вытяжка De Dietrich DHT3622X\n",
    "Духовые шкафы\n",
    "Духовые шкафы\n",
    "Компактные духовые шкафы\n",
    "Духовые шкафы с паром\n",
    "Компактные духовые шкафы с паром\n",
    "Паровые шкафы\n",
    "Варочные панели\n",
    "Индукционные варочные панели\n",
    "Варочные панели Домино\n",
    "Электрические варочные панели\n",
    "Газовые варочные панели\n",
    "Комбинированные варочные панели\n",
    "Вытяжки\n",
    "Настенные вытяжки\n",
    "Встраиваемые вытяжки\n",
    "Островные вытяжки\n",
    "Встраиваемые в столешницу вытяжки\n",
    "Холодильные и морозильные шкафы\n",
    "Встраиваемый холодильный шкаф\n",
    "Винные шкафы\n",
    "Встраиваемый морозильный шкаф\n",
    "Встраиваемый холодильно-морозильный шкаф\n",
    "Отдельностоящие холодильные шкафы\n",
    "Посудомоечные машины\n",
    "Микроволновые печи\n",
    "Кофемашины\n",
    "Вакууматоры и подогреватели\n",
    "Стирально-сушильная техника\n",
    "Аксессуары\n",
    "Outlet и НКТ\n",
    "Встраиваемая вытяжка De Dietrich DHT3622X\n",
    "ALT\n",
    "ALT\n",
    "ALT\n",
    "Варианты дизайна\n",
    "Встраиваемая вытяжка De Dietrich DHT3622X\n",
    "Встраиваемая вытяжка De Dietrich DHT3622X\n",
    "Сравнение\n",
    "62 990 ₽\n",
    "Товар под заказ\n",
    "Доставка: БЕСПЛАТНО ДО КВАРТИРЫ\n",
    "заказ\n",
    "\n",
    "\n",
    "\n",
    "Описание\n",
    "Характеристики\n",
    "Документы\n",
    "Встраиваемая вытяжка De Dietrich DHT3622X\n",
    "\n",
    "Механическое управление с курсором\n",
    "Маркировка энергопотребления: класс B\n",
    "Функция \"Вкл./Выкл.\"\n",
    "Функция \"Boost\"\n",
    "Максимальная производительность: 425 м3/ч\n",
    "Уровень шума: мин: 57 дБ(A), макс.: 69 дБ(A)\n",
    "3 уровня мощности, включая Boost\n",
    "Независимое освещение\n",
    "2 x 2,5 Вт светодиодное освещение\n",
    "\n",
    "\n",
    "Посмотреть все вытяжки коллекции PLATINUM\n",
    "\n",
    "Посмотреть все товары коллекции PLATINUM\n",
    "\n",
    "\n",
    "\n",
    "Режим BOOST\n",
    "Режим BOOST\n",
    "Выбирая этот режим вы мгновенно устанавливаете максимальную производительность, чтобы самым эффективным образом удалять большие количества испарений.\n",
    "Светодиодное освещение\n",
    "Светодиодное освещение\n",
    "Этот метод освещения выделяется тем, что потребляет почти на 85% меньше электроэнергии и служит в 20 раз дольше, чем лампа накаливания. Система эффективно освещает все кастрюли на вашей плите и обеспечивает вам мягкое освещение, идеально подходящее для ужина.\n",
    "\n",
    "\n",
    "\n",
    "zakaz@dedietrich-shop.ru\n",
    "\n",
    "Мы в соцсетях:\n",
    "TELEGRAM\tВКонтакте\n",
    "©2026. Магазин французской бытовой техники De Dietrich. Все права защищены\n",
    "\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        for text in test_texts:\n",
    "            print(f\"\\nВход: {text}\")\n",
    "            result = test_generation(model, tokenizer, text)\n",
    "            print(f\"Результат:\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6763a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ГЕНЕРАЦИЯ ТЕКСТА С ОБУЧЕННОЙ МОДЕЛЬЮ T5-LoRA\n",
      "============================================================\n",
      "Загрузка модели из fast_fine_tuned_t5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "c:\\Users\\loban\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена на устройство: cpu\n",
      "Размер модели: 65,332,608 параметров\n",
      "Прочитан файл: ../rag_data/Pages/inputs/page_1_DHT3622X.txt\n",
      "Длина текста: 2114 символов\n",
      "\n",
      "Генерация текста...\n",
      "\n",
      "============================================================\n",
      "РЕЗУЛЬТАТЫ:\n",
      "============================================================\n",
      "\n",
      "Входной текст (первые 300 символов):\n",
      "----------------------------------------\n",
      "Официальный дистрибьютор\n",
      "0\n",
      "\n",
      "Я ищу...\n",
      "Главная \n",
      "Каталог \n",
      "Вытяжки \n",
      "Встраиваемые вытяжки \n",
      "Встраиваемая вытяжка De Dietrich DHT3622X\n",
      "Духовые шкафы\n",
      "Духовые шкафы\n",
      "Компактные духовые шкафы\n",
      "Духовые шкафы с паром\n",
      "Компактные духовые шкафы с паром\n",
      "Паровые шкафы\n",
      "Варочные панели\n",
      "Индукционные варочные панели\n",
      "Вароч...\n",
      "\n",
      "\n",
      "Сгенерированный текст:\n",
      "----------------------------------------\n",
      "Стирально-сушильная техника Аксессуары Оutlet и НКТ встраиваемые вытяжки Встраиваемые в столешницу вытяжки Холодильные и морозильные шкафы Посудомоечные машины Микроволновые печи Кофемашины Вакууматоры и подогреватели Стирально-сушильная техника Аксессуары Оutlet и НКТ Встраиваемые вытяжки Встраиваемые вытяжки Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые вытяжки Холодильные и морозильные шкафы Встраиваемые встраиваемые\n",
      "\n",
      "Результат сохранен в: output.txt\n",
      "Длина сгенерированного текста: 865 символов\n",
      "\n",
      "============================================================\n",
      "ГЕНЕРАЦИЯ ЗАВЕРШЕНА УСПЕШНО!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from pathlib import Path\n",
    "\n",
    "# =============== КОНФИГУРАЦИЯ ===============\n",
    "MODEL_PATH = \"fast_fine_tuned_t5\"  # Путь к обученной модели\n",
    "INPUT_FILE = \"../rag_data/Pages/inputs/page_1_DHT3622X.txt\"  # Входной файл\n",
    "OUTPUT_FILE = \"output.txt\"  # Выходной файл\n",
    "\n",
    "# Параметры генерации\n",
    "GENERATION_PARAMS = {\n",
    "    \"max_length\": 256,\n",
    "    \"num_beams\": 4,\n",
    "    \"temperature\": 0.7,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "}\n",
    "\n",
    "# =============== ЗАГРУЗКА МОДЕЛИ С LoRA ===============\n",
    "def load_lora_model(model_path):\n",
    "    \"\"\"Загрузка модели с адаптерами LoRA\"\"\"\n",
    "    print(f\"Загрузка модели из {model_path}...\")\n",
    "    \n",
    "    # Загружаем конфигурацию LoRA\n",
    "    peft_config = PeftConfig.from_pretrained(model_path)\n",
    "    \n",
    "    # Загружаем базовую модель\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path\n",
    "    )\n",
    "    \n",
    "    # Загружаем модель с адаптерами LoRA\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    # Загружаем токенизатор\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Переводим модель в режим инференса\n",
    "    model.eval()\n",
    "    \n",
    "    # Перемещаем на GPU если доступно\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Модель загружена на устройство: {device}\")\n",
    "    print(f\"Размер модели: {model.num_parameters():,} параметров\")\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "# =============== ЧТЕНИЕ ВХОДНОГО ТЕКСТА ===============\n",
    "def read_input_text(file_path):\n",
    "    \"\"\"Чтение текста из файла\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "        print(f\"Прочитан файл: {file_path}\")\n",
    "        print(f\"Длина текста: {len(text)} символов\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка чтения файла: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============== ГЕНЕРАЦИЯ ТЕКСТА ===============\n",
    "def generate_text(model, tokenizer, device, input_text, prefix=\"summarize: \"):\n",
    "    \"\"\"Генерация текста моделью\"\"\"\n",
    "    # Добавляем префикс (такой же, как при обучении)\n",
    "    processed_text = prefix + input_text\n",
    "    \n",
    "    # Токенизируем входной текст\n",
    "    inputs = tokenizer(\n",
    "        processed_text,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Перемещаем на то же устройство, что и модель\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Генерируем текст\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **GENERATION_PARAMS\n",
    "        )\n",
    "    \n",
    "    # Декодируем результат\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# =============== СОХРАНЕНИЕ РЕЗУЛЬТАТА ===============\n",
    "def save_output(text, file_path):\n",
    "    \"\"\"Сохранение сгенерированного текста в файл\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"\\nРезультат сохранен в: {file_path}\")\n",
    "        print(f\"Длина сгенерированного текста: {len(text)} символов\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка сохранения файла: {e}\")\n",
    "        return False\n",
    "\n",
    "# =============== ОСНОВНАЯ ФУНКЦИЯ ===============\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ГЕНЕРАЦИЯ ТЕКСТА С ОБУЧЕННОЙ МОДЕЛЬЮ T5-LoRA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Проверяем наличие файлов\n",
    "    if not Path(MODEL_PATH).exists():\n",
    "        print(f\"Ошибка: модель не найдена по пути {MODEL_PATH}\")\n",
    "        return\n",
    "    \n",
    "    if not Path(INPUT_FILE).exists():\n",
    "        print(f\"Ошибка: входной файл не найден {INPUT_FILE}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Загружаем модель\n",
    "    model, tokenizer, device = load_lora_model(MODEL_PATH)\n",
    "    \n",
    "    # 3. Читаем входной текст\n",
    "    input_text = read_input_text(INPUT_FILE)\n",
    "    if not input_text:\n",
    "        return\n",
    "    \n",
    "    # 4. Генерируем текст\n",
    "    print(\"\\nГенерация текста...\")\n",
    "    generated_text = generate_text(model, tokenizer, device, input_text)\n",
    "    \n",
    "    # 5. Выводим результаты\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"РЕЗУЛЬТАТЫ:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nВходной текст (первые 300 символов):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(input_text[:300] + \"...\" if len(input_text) > 300 else input_text)\n",
    "    \n",
    "    print(f\"\\n\\nСгенерированный текст:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(generated_text)\n",
    "    \n",
    "    # 6. Сохраняем результат\n",
    "    save_output(generated_text, OUTPUT_FILE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ГЕНЕРАЦИЯ ЗАВЕРШЕНА УСПЕШНО!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# =============== ЗАПУСК ===============\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78645b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def extract_product_info(file_path):\n",
    "    # Читаем содержимое файла\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_content = file.read()\n",
    "    \n",
    "    # Подготавливаем системное сообщение и пользовательский запрос\n",
    "    system_message = \"Ты помощник для извлечения информации о товаре из текста веб-страницы.\"\n",
    "    user_query = \"\"\"Перед тобой текст, скопированный с веб-страницы карточки товара, убери из него все артефакты сайта типа хэдера, падера и пунктов меню, чтобы осталось только наименование товара в первой строке и разделённые переносами строк свойства товара\n",
    "\n",
    "Вот текст:\n",
    "{text}\"\"\".format(text=text_content)\n",
    "    \n",
    "    # Подключаемся к API\n",
    "    client = OpenAI(\n",
    "        api_key=\"nK1FfIy_yI90TPVqIafoc7Pd38i-gBD6\",\n",
    "        base_url=\"https://chat.immers.cloud/v1/endpoints/gpt-oss-20b/generate/\",\n",
    "    )\n",
    "    \n",
    "    # Отправляем запрос\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-oss-20b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ],\n",
    "        temperature=0.1  # Низкая температура для более детерминированного ответа\n",
    "    )\n",
    "    \n",
    "    # Возвращаем результат\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "# Использование\n",
    "result = extract_product_info(\"../rag_data/Pages/inputs/page_1_DHT3622X.txt\")  # Укажите путь к вашему файлу\n",
    "with open(\"output.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
